\documentclass[10pt,a4paper]{article}
\usepackage[paper=a4paper,hmargin=0.5cm,bottom=1.5cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{xargs}
\usepackage{xspace}
\usepackage{caratula}
\usepackage{ifthen}
\usepackage{aed2-tad,aed2-symb,aed2-itef,aed2-diseno}
\usepackage{aed2-tad,aed2-symb,aed2-itef}
\usepackage{algorithmicx, algpseudocode, algorithm}
\usepackage{graphicx}
\usepackage{graphicx} 
\graphicspath{ {./images/} }
\usepackage[rightcaption]{sidecap}
\usepackage{wrapfig}

\materia{Algoritmos y Estructuras de Datos II}
\titulo{Apunte examen final}
\subtitulo{Segundo cuatrimestre 2021}
\integrante{Ezequiel Rueda Sanchez}{522/16}{ezequiel.ruedasanchez@gmail.com}

%  ü, é, á, í, ó, ú, ñ, Ñ%


\begin{document}

\maketitle

\section{Clase Teórica 01. Especificación I}

\subsection{Introducción}

Típicamente, nos enfrentamos a la siguiente situación. Tenemos un $problema$ que no es presentado en una manera difusa, vaga y se pretende que lo podamos resolver a traves de una computadora de forma eficiente. Lo problematico es transitar este recorrido que va desde la definición informal del problema a su resolución computacional. Este camino tiene una serie de pasos que los llamamos \textbf{especificación}, \textbf{diseño} e \textbf{implementación}. 

%\begin{figure}[h]
%	\centering
%	\includegraphics[width=0.7\textwidth]{etapas-res-problem}
%	\caption{Etapas en la resolucion de un problema}
%	\label{drivers1}
%\end{figure}

\subsection{Contexto}

Un \textbf{algoritmo} es un procedimiento para resolver un problema, descripto por una secuencia ordenada y finita de pasos bien determinados que nos llevan de un estado inicial a uno final.
\newline
\newline
Ahora bien, el algoritmo, más alla de ser un objeto en si mismo, es una herramienta para resolver problemas donde los más complejos se van a resolver combinando parte de ellos y se van a poder modificar para resolver distintas variantes o \textbf{problemas genéricos}.   
\newline
\newline
Entonces, ¿Qué es resolver un problema \textit{genérico}?
\newline
\newline
Se trata de, dada su descripción, proponer un algoritmo que resuelva cualquier instancia del mismo. Por ejemplo, no proponer un algoritmo que resuelva la suma de 7 y 5 sino proponer un algoritmo que resuelva la suma de dos números enteros cualesquiera.  
\newline
\newline
Para ello, necesitamos herramientas que nos ayuden a transitar el camino desde el enunciado informal hasta la implementación en la computadora. Sobre ello vamos a trabajar a lo largo del curso.

\subsection{Resolución de problemas}

El dueño de un restaurant quiere asegurarse de que los pedidos sean atendidos con prolijidad. Los mozos llevan los pedidos hasta la cocina donde los colocan. Cuando el cocinero se libera, saca el primer pedido y prepara el plato indicado. El dueño quiere saber cuál es el próximo plato a preparar, cuántos pedidos atiende el cocinero cada día y cuál fue el día con menos pedidos.
\newline
\newline
No parece que el problema se pueda expresar directamente en un lenguaje matemático, pero está claro que necesitamos algo riguroso y formal para poder pasar al algoritmo. 
\newline
\newline
Sin embargo, podemos observar que se pretenden obtener resultados de varios tipos distintos: al menos naturales (los dias que pasan) y platos. Además, hay varias operaciones asociadas a estos tipos.
\newline
\newline
Para poder modelar este problema, vamos a usar una herramienta nueva: \textbf{Tipos Abstracto de Datos (TAD)}. Veamos un poco más en detalle este concepto.
\newline
\newline
\textbf{.} Tipo: conjunto de valores asociados a operaciones
\newline
\newline
\textbf{.} Tipo abstracto: no nos interesa tanto "la forma" de los valores sino saber que operaciones podemos realizar sobre ellos donde la única manera de obtener información sobre ellos es a través de las operaciones que definan.
\newline
\newline
El problema maneja platos, dias y (al menos un) restaurant.
\newline
\newline
¿Qué nos interesa saber de cada una de estas $"$cosas$"$?
\newline
\newline
\textbf{.} Los dias pasan por lo que vamos a querer contarlos.
\newline
\newline
\textbf{.} Solo nos interesa poder diferenciar los platos entre si.
\newline
\newline
\textbf{.} De los restaurants, varias cosas, asi que lo dejamos de lado por un momento.
\newpage
Comenzamos usando renombres:
\newline
\newline
\textbf{.} TAD \tadNombre{DIA} es \tadNombre{NAT}  
\newline
\newline
\textbf{.} TAD \tadNombre{PLATO} es \tadNombre{STRING}.
\newline
\newline
¿Por qué tenemos que renombrar estos TADs y no utilizar STRING o NAT directamente? Lo que queriamos hacer era utilizar un lenguaje de especificación que nos permita abstraer ciertos conceptos, por lo que usar STRING directamente no sería correcto. Para el restaurant no tiene sentido un STRING, pero los PLATOS si. De esta manera, nos alejamos de la manera en la que representamos los datos en una computadora y nos acercamos a la definición del problema.
\newline
\newline
Pasemos ahora al restaurant...
\newline
\begin{tad}{\tadNombre{Restaurant}}
	\medskip	
	\textbf{generos} restaurant
	\newline
	
	\textbf{operaciones}
	\medskip
	\tadOperacion{inaugurar}{}{restaurant}{}
	\medskip
	\tadOperacion{cantPlatosPendientes}{restaurant}{nat}{}
	\medskip
	\tadOperacion{proximoPedido}{restaurant/r}{plato}{cantPlatosPendientes($r$) $>$ 0}
	\medskip
	\tadOperacion{prepararPlato}{restaurant/r}{restaurant}{cantPlatosPendientes($r$) $>$ 0}
	\medskip
	\tadOperacion{tomarPedido}{restaurant, plato}{restaurant}{}
	\medskip
	\tadOperacion{nuevoDia}{restaurant}{restaurant}{}
	\medskip
	\tadOperacion{diaActual}{restaurant}{dia}{}
	\medskip
	\tadOperacion{platosPorDia}{restaurant/r, dia/d}{nat}{$d$ $\leq$ diaActual($r$)}
	\medskip
	\tadOperacion{diaMenosPedidos}{restaurant}{dia}{}
	\medskip
\end{tad}
\medskip
A esto lo llamamos \textbf{signatura} del TAD. La signatura nos indica que operaciones tiene el tipo, con qué parámetros y qué devuelven.
\newline
\newline
Las operaciones de los TAD's son \textit{funciones totales}. O sea, funciones que están definidas para cada valor del dominio. Por eso, en casos como \TipoVariable{preparar\_plato()} debemos restringir el dominio.
\newline
\newline
Ahora, necesitamos darle \textbf{semántica}, es decir, comportamiento a las operaciones. En los TADs usaremos para eso los \textbf{axiomas}.
\newline
\newline
Entonces, veamos los axiomas del TAD \tadNombre{RESTAURANT}.
\newline
\begin{tad}{\tadNombre{Restaurant}}
\medskip
\tadAxiomas[$\forall$ $r$ : \tadNombre{Restaurant}, $p$ : \tadNombre{Plato}, $d$ : \tadNombre{Dia}]
\medskip
\tadAlinearAxiomas{diaActual(tomarPedido(r,p))}
\medskip
\tadAxioma{diaActual(inaugurar())}{0}
\medskip
\tadAxioma{diaActual(nuevoDia(r))}{diaActual(r)+1}
\medskip
\tadAxioma{diaActual(tomarPedido(r,p))}{diaActual(r)}
\medskip
\medskip

\tadAlinearAxiomas{cantPlatosPendientes(tomarPedido(r,p))}
\medskip
\tadAxioma{cantPlatosPendientes(inaugurar())}{0}
\medskip
\tadAxioma{cantPlatosPendientes(tomarPedido(r,p))}{cantPlatosPendientes(r)+1}
\medskip
\tadAxioma{cantPlatosPendientes(prepararPlato(r))}{cantPlatosPendientes(r)-1}
\medskip
\medskip

\tadAlinearAxiomas{cantPlatosPendientes(tomarPedido(r,p))}
\medskip
\tadAxioma{proximoPedido(r)}{ult(secuenciaDePedidos(r))}
\medskip
\medskip

\tadAlinearAxiomas{secuenciaDePedidos(tomarPedido(r,p))}
\medskip
\tadAxioma{secuenciaDePedidos(inaugurar())}{$\secuvacia$}
\medskip
\tadAxioma{secuenciaDePedidos(tomarPedido(r,p))}{p $\puntito$ secuenciaDePedidos(r)}
\medskip
\tadAxioma{secuenciaDePedidos(prepararPlato(r))}{com(secuenciaDePedidos(r))}
\medskip
\medskip

\tadAlinearAxiomas{platosPorDia(d, tomarPedido(r,p))}
\medskip
\tadAxioma{platosPorDia(d, inaugurar())}{0}
\medskip
\tadAxioma{platosPorDia(d, tomarPedido(r,p))}{platosPorDia(d,r)}
\medskip
\tadAxioma{platosPorDia(d, prepararPlato(r))}{\IF diaActual(r) = d THEN platosPorDia(d,r)+1 ELSE platosPorDia(d,r) FI}
\medskip
\tadAxioma{platosPorDia(d, nuevoDia(r))}{\IF diaActual(r)+1 = d THEN 0 ELSE platosPorDia(d,r) FI}
\medskip
\medskip

Por ultimo, vamos a axiomatizar la funcion diaMenosPedidos(r) de la siguiente forma:
\newline
\newline
($\forall$ $d'$:dia) (0 $\leq$ $d'$ $\leq$ diaActual($r$) $\impluego$ platosPorDia($r$, diaMenosPedidos($r$)) $\leq$ platosPorDia($r,d'$))
\medskip
\end{tad}

\subsection{TAD's}

¿Qué es un TAD?
\newline
\newline
\textbf{.} Desde el punto de vista formal, es una herramienta lógico/matematica. Eso es bueno porque nos da la rigurosidad que necesitamos para entender claramente qué hay que hacer.
\newline
\newline
\textbf{.} Desde el punto de vista práctico, es una herramienta poderosa y flexible que nos permite resolver o formalizar infinidad de problemas.
\newline
\newline
\textbf{.} Desde el punto de vista histórico, uno de los primeros intentos por abordar este problema.
\newline
\newline
Una de las ventajas de la teoría de los tipos abstractos de datos es que no requiere de tipos primitivos que deban definirse por fuera de la misma. 

\subsection{Secciones de un TAD}

Las secciones de un TAD son:
\newline
\newline
\textbf{.} Géneros: Los géneros (en general va a haber sólo uno, pero podrían ser más) son el nombre que recibe el conjunto de valores del tipo. Pensar en el monoide conmutativo ($\mathbb{N}$, +) y en el conjunto de los números naturales $\mathbb{N}$. 
\newline
\newline
\textbf{.} Usa: Inclusión de los géneros y operaciones definidos en otros tipos abstractos de datos que utiliza el tipo que estamos definiendo.
\newline
\newline
\textbf{.} Exporta: Qué operaciones y géneros se dejan a disposición de los usuarios del tipo.
\newline
\newline
\textbf{.} Generadores: Son operaciones que permiten construir instancias del tipo. Un conjunto de generadores está bien definido si una combinación de ellos permite construir cualquier instancia posible del tipo.
\newline
\newline
\textbf{.} Observadores: Son operaciones que nos permiten, utilizadas en conjunto, diferenciar instancias del tipo.
\newline
\newline
\textbf{.} Axiomas: Son las reglas que nos explican el comportamiento de las funciones o describen el comportamiento desde el punto de vista semántico de un objeto del TAD. 
\newpage

\section{Clase Teórica 02. Especificación II}

\subsection{Introducción}

La especificación con TADs no es la única forma de hacerlo.
\newline
\newline
Siempre es posible la utilización de \textit{métodos informales}
\newline
\newline
Los métodos informales tienen un menor costo inicial (son mas fáciles de construir y utilizar) pero justamente por ello, no permiten encontrar errores e inconsistencias en la especificación, ni la aplicación de técnicas automáticas para la validación (en contraposición a los métodos formales con la inducción estructural por ejemplo), etc.
\newline
\newline
El costo de reparar un defecto aumenta con el tiempo entre la introducción y el descubrimiento.
\newline
\newline
En cambio, los \textit{métodos formales} no cuentan con esos problemas pero existen de variado tipo y color con ventajas y desventajas para cada caso. Veamos alguno de ellos:
\newline
\newline
\textbf{.} Algebraicos: el sistema se especifica mediante la definición de un álgebra con sus términos, operaciones, axiomas, etc. Los TADs entran en esta categoría.
\newline
\newline
\textbf{.} Operacionales: la especificación se realiza en un lenguaje imperativo de alto nivel.
\newline
\newline
\textbf{.} Basados en estados: el sistema se modela como un conjunto de estados posibles y las relaciones entre ellos. 

\subsection{Resolución de problemas}

Retomando el ejercicio del \textit{restaurant}, comenzamos con una descripción del problema y terminamos con el modelao del mismo. 
\newline
\newline
Pero, ¿Qué ocurrió en el medio?
\newline
\newline
¿Qué pasó con, por ejemplo, el dueño del restaurant? 
\newline
\newline
El dueño no está porque no es necesario que esté. Utilizamos un proceso llamado \textbf{abstracción}. La abstracción consiste en identificar los aspectos importantes de un fenomeno e ignorar el resto de los detalles.
\newline
\newline
También utilizamos otro concepto: el de \textbf{modularidad}. Los TADs se incluyen de manera tal que no sea necesario (ni conveniente) escribir una gran especificación que mezcle todo y dificulte la comprensión. 
\newline
\newline
El restaurant que especificamos, ¿es eficiente?
\newline
\newline
La pregunta no tiene sentido en una etapa de especificación porque justamente la especificación define que es lo que tenemos que computar, que es lo que tenemos que calcular para resolver nuestro problema y no como lo vamos a hacer. 
\newline
\newline
Nos interesa describir el \textbf{el qué} y no \textbf{el cómo}.

\subsection{Axiomatización}

Las operaciones son funciones, asi que debemos evitar inconsistencias. Para lograr esto, evitaremos la \textbf{sobre especificación}. Es decir, cuando tenemos varias formas de decir lo mismo.
\newline
\newline
Veamos la \textit{sobre especificación} con un ejemplo.
\newline
\newline
\textbf{.} \TipoVariable{prim\_excepto\_3(3 $\puntito$ S)} $\equiv$ 21
\newline
\newline
\textbf{.} \TipoVariable{prim\_excepto\_3(a $\puntito$ S)} $\equiv$ a
\newline
\newline
En este ejemplo, quiero devolver el primer elemento de la secuencia \TipoVariable{S} excepto que cuando el primer elemento es un 3, devuelvo cualquier número arbitrario, por ejemplo, el 21. 
\newline
\newline
Observemos que estamos haciendo una especie de \textit{pattern matching} porque si la secuencia comienza con 3, elijo la primer opción y en cualquier otro caso, la segunda. Pero tenemos valores contradictorios en el caso que el primer elemento de la secuencia sea el 3 porque nada nos asegura que vaya por la primer o segunda opción.
\newline
\newline
Tampoco podemos especificar sobre los casos restringidos ya que están fuera del dominio.
\newline
\newline
No debemos \textbf{subespecificar}. Es decir, dejar de lado y no dar definiciones precisas de ciertos aspectos particulares de un determinado axioma. Veamos el siguiente ejemplo.
\newline
\tadOperacion{premiar}{torneo/t, jugador/j}{premio}{}
\medskip
\tadAlinearAxiomas{premiar(t,j)}
\medskip
\tadAxioma{premiar(t,j)}{seleccionarMejorPremio(premiosDisponibles(t))}
\medskip
\tadAxioma{premiar(t,j)}{dameUno(premiosDisponibles(t))}
\medskip
\medskip
Observemos que la segunda axiomatización es mucho más debil (está \textit{subespecificada}) que la primera dado que ésta es mucho mas precisa porque pide el mejor premio mientras que la segunda simplemente devuelve un premio. 
\newline
\newline
Entonces, ¿Hasta cuánto debemos axiomatizar?
\newline
\newline
Una regla práctica consiste en axiomatizar todos los observadores básicos sobre todos los generadores no restringidos, para asegurar que cubrimos todo el dominio.
\newline
\newline
En general, aunque podría haber excepciones, el resto de las operaciones deberían poder escribirse en base a los observadores básicos.

\subsection{Algunas libertades}

Si usamos todo lo que aparece mencionado, no es necesario escribir la clausula \textbf{usa}.
\newline
\newline
El \textbf{exporta} tiene un valor por omisión: los géneros, los observadores básicos y los generadores. Además, cuando se exporta algo más, alcanza con aclarar qué es lo extra que se exporta.
\newline
\newline
En la \textbf{axiomatización}:
\newline
\newline
\textbf{.} Todas las variables libres se suponen universalmente cuantificadas.
\newline
\newline
\textbf{.} Sólo cuantificamos explícitamente cuando podría generarse confusión.

\subsection{Elección de generadores y observadores}

Conviene que el conjunto de observadores y generadores sean \textit{minimales}. 
\newline
\newline
Si los generadores no lo fuesen, se corre el riesgo de producir inconsistencias.
\newline
\newline
Idem con los observadores. Además, la redundancia atenta contra la claridad.
\newline
\newline
Los lenguajes naturales tienen una \textit{semántica difusa} mientras que los \textit{lenguajes formales} de especificación son rigurosos, o dicho de otra manera, tienen una \textit{semántica precisa}. A esta diferencia se la conoce como \textbf{semantic gap} o \textbf{brecha semántica}.
\newline
\newline
Como primer paso para vincular los mundos \textit{informales} con los \textit{formales} utilizaremos una herramienta llamada \textbf{igualdad observacional}.
\newpage

\subsection{Igualdad observacional (como herramienta)}

La igualdad observacional es un predicado entre instancias del tipo que nos dice cuando son iguales (desde el punto de vista de su comportamiento).
\newline
\newline
Notemos que \TipoVariable{Ag(1, Ag(2, $\emptyset$))} es sintácticamente distinto a \TipoVariable{Ag(2, Ag(1, $\emptyset$))} aunque desde el punto de vista de comportamiento nos gustaría que sean iguales. 
\newline
\newline
Entonces, para poder comparar las instancias del ejemplo anterior vamos a utilizar la igualdad observacional que se va a valer de una serie de funciones (los observadores básicos) los cuales van a prestar atención a detalles particulares de esas instancias para poder compararlas.
\newline
\newline
Por lo tanto, la igualdad observacional nos permite:
\newline
\newline
\textbf{.} Deducir cuáles son los observadores necesarios.
\newline
\newline
\textbf{.} Explicitar cómo se combinan en un único predicado.

\subsection{Congruencia}

Imaginemos el TAD \tadNombre{Negocio}.
\newline
\begin{tad}{\tadNombre{Negocio}}
	\medskip	
        \textbf{generadores}
	\medskip
	\tadOperacion{inaugurar}{}{negocio}{}
	\medskip
	\tadOperacion{vender}{negocio, producto}{negocio}{}
	\medskip
        \medskip
        \textbf{observadores basicos}
        \medskip
	\tadOperacion{total\_vendido}{negocio}{nat}{}
	\medskip
        \medskip
        \textbf{otras operaciones}
        \medskip
\tadOperacion{ventas}{negocio}{secu(producto)}{}
	\medskip
\end{tad}
\medskip
\medskip
De acuerdo con esa especificación dos negocios van a ser iguales si su total vendido coincide. Sin embargo, podría haber instancias cuyo total vendido coincida pero la secuencia de productos que devuelve la operación ventas no.
\newline
\newline
Formalmente, la función \TipoVariable{ventas()} rompe la \textbf{congruencia}. Es decir, diferencia elementos que quedan en la misma clase de equivalencia de acuerdo con la igualdad observacional (o lo que es lo mismo, de acuerdo con los observadores básicos).
\newline
\newline
Desde el punto de vista de modelado, o bien desaparece \TipoVariable{ventas()} o bien se vuelve un observador básico, y por ende, miembro de la igualdad observacional.
\newline
\newline
Si no hacemos eso, se genera una \textbf{inconsistencia}: hay dos instancias del TAD \tadNombre{Negocio} que son equivalentes según los observadores básicos, pero que son diferenciables de acuerdo a los axiomas efectivamente escritos. 
\newpage

\section{Clase Teórica 03. Complejidad Algorítmica}

Para resolver problemas, tenemos que diseñar algoritmos y estructuras de datos que sean \textbf{eficientes}  en términos del consumo de recursos.
\newline
\newline
Esa medida de eficiencia nos permitirá elegir entre distintos algoritmos para resolver el mismo problema y distintas formas de implementar un TAD. 
\newline
\newline
¿Cuáles son esos recursos que se consumen?
\newline
\newline
\textbf{.} Tiempo de ejecución
\newline
\newline
\textbf{.} Espacio (memoria)
\newline
\newline
\textbf{.} Cantidad de procesadores (en el caso de algoritmos paralelos)
\newline
\newline
\textbf{.} Utilización de la red de comunicaciones (para algoritmos paralelos).
\newline
\newline
Nos vamos a ocupar de los primeros dos criterios: tiempo de ejecución y espacio (memoria).

\subsection{Complejidad Algorítmica}

¿Cómo medimos la complejidad algorítmica? El análisis de la complejidad de un algoritmo se puede hacer a traves de dos enfoques:
\newline
\newline
\textbf{.} Empírica o experimental
\newline
\newline
Medir el tiempo de ejecución para una determinada entrada y en una computadora concreta
\newline
\newline
Usando un cronómetro, o analizando el consumo de recursos de la computadora (tiempo de CPU)
\newline
\newline
Medidas del tipo: 3GB, 1.5 segundos.
\newline
\newline
\textbf{.} Teórica
\newline
\newline
Medida teórica del comportamiento de un algoritmo. Es decir, la idea es poder calcular la complejidad a priori, intuir, poder estimar lo que va a tardar la complejidad de un algoritmo antes de ejecutarlo.
\newline
\newline
Uno de los problemas que tiene el análisis empírico sobre el teórico es que medir el tiempo de ejecución me puede insumir mucho tiempo en realizarlo. Además, el análisis empírico cambia de computadora en computadora y otro problema es que el contexto de uso puede cambiar mucho el tiempo real de ejecución. Si estoy ejecutando el algoritmo en un sistema multitasking y demoró una cierta cantidad de tiempo, no implica que va a tardar lo mismo en un entorno con muchos más usuarios logeados, problemas en la conexión de la red o cualquier factor externo pero que influye en la performance de mi equipo.

\subsection{Ventajas del enfoque teórico}

Las ventajas del enfoque teórico son:
\newline
\newline
\textbf{.} El análisis se puede hacer a priori, aún antes de escribir una linea de código.
\newline
\newline
\textbf{.} Vale para todas las instancias del problema
\newline
\newline
\textbf{.} Es independiente de la máquina en la que se ejecuta
\newline
\newline
\textbf{.} Es independiente de la pericia del programador
\newpage

\subsection{Análisis teórico}

La medida de complejidad está asociada a un $"$módelo de máquina$"$ o $"$módelo de cómputo$"$ consensuado.
\newline
\newline
Para medir todas las instancias posibles, no vamos a hablar de instancias particulares sino de tamaño de las instancias. Vamos a dar una medida de complejidad en función del \textit{input} de las instancias. 
\newline
\newline
Puede suceder que no todas las instancias del mismo tamaño consuman la memoria de la misma manera. Entonces, para esos casos, vamos a ver complejidad para distintos tipos de inputs. 
\newline
\newline
Vamos a realizar \textit{análisis asintòtico}. Nos vamos a enfocar en el tiempo de ejecución de instancias grandes.

\subsubsection{Modelo cómputo}

Queremos una medida \textit{universal} válida para distintas implementaciones del algoritmo.
\newline
\newline
Por lo tanto, vamos a inventar una máquina teórica que vamos a utilizar como banco de pruebas para la ejecución del algoritmo. 
\newline
\newline
En esta definición de máquina, vamos a poder tomar una \textbf{medida del tiempo}: número de pasos o instrucciones que se ejecutan en esa máquina \textit{ideal} para determinado \textit{input}.
\newline
\newline
En esta definición de máquina, vamos a poder tomar una \textbf{medida del espacio}: número de posiciones de memoria en esa máquina \textit{ideal} que se utilizan para determiando \textit{input}.
\newline
\newline
\textbf{Operaciones elementales}
\newline
\newline
Las operaciones elementales son aquellas operaciones que tardan una unidad de tiempo en el modelo de máquina que estoy definiendo.
\newline
\newline
Vamos a utilizar una función de complejidad \textit{t(I)} que mide el número de operaciones elementales requeridas para la instancia \textit{I}.
\newline
\newline
Las operaciones elementales (OE) serán aquellas que el procesador realiza en tiempo acotado por una constante (que no depende del tamaño de la entrada).
\newline
\newline
Consideramos OE las operaciones aritméticas básicas, comparaciones lógicas, transferencias de control, asignaciones a variables de tipos básicos, etc (tener cuidado; para ello es importante definir bien el modelo de cómputo y cuáles son las operaciones elementales).
\newline
\newline
\textbf{Cálculo de Operaciones Elementales}
\newline
\newline
\textbf{.} Vamos a considerar que el tiempo de una OE es, por definición, 1.
\newline
\newline
\textbf{.} El tiempo de ejecución de una secuencia consecutiva de instrucciones se calcula sumando los tiempos de ejecución de cada una de las instrucciones.
\newline
\newline
\textbf{Ejemplo}.

\begin{algorithm}[H]{i\tadNombre{busquedaSecuencial}(\In{A}{array(T)}, \In{x}{T})}
	\begin{algorithmic}[1]
		\State $i$ $\gets$ 1
            \State $encontre$ $\gets$ $false$
            \While{$!encontre$}
		\If{$A[i] = x$}
		\State $encontre$ $\gets$ $true$                 
		\State $i$ $\gets$ $i+1$         \EndIf
            \EndWhile
            \State $print(i-1)$ 
		
		\medskip
	\end{algorithmic}
\end{algorithm}
\newpage

¿Cuánto tarda la ejecución de $Buscar(5, [2,6,3,5,8])$?
\newline
\newline
Para resolver esto, tenemos que contar cuantas operaciones elementales se ejecutan entre que empieza el algoritmo y termina con una respuesta. 
\newline
\newline
Comienza el algoritmo y se realiza la asignación $i$ $\gets$ 1 (1OE). Luego, se realiza la asignación $encontre$ $\gets$ $false$ (1OE).
\newline
\newline
En la instrucción \textbf{while} $!encontre$, tenemos una negación y una comparación (2OE). 
\newline
\newline
Luego, en la instrucción \textbf{if} $A[i] = x$ tenemos 1OE para acceeder a la variable $x$, otra OE para compararlo con $A[i]$. Ahora, como 5 != 2, no accedo a la linea de $encontre$ $\gets$ $true$ pero si a la de $i$ $\gets$ $i+1$ que hay 3OE, 1 para el acceso de la variable $i$, otra para la suma de esta y una última para el salto al ciclo \textbf{while} nuevamente. 
\newline
\newline
Analogamente al caso anterior porque 6 != 5, tenemos 2OE con respecto a la instrucción \textbf{while} $!encontre$, 2OE con respecto a la instrucción \textbf{if} $A[i] = x$ y 3OE con respecto a la instrucción $i$ $\gets$ $i+1$. 
\newline
\newline
Del mismo modo como 3 != 5, , tenemos 2OE con respecto a la instrucción \textbf{while} $!encontre$, 2OE con respecto a la instrucción \textbf{if} $A[i] = x$ y 3OE con respecto a la instrucción $i$ $\gets$ $i+1$.
\newline
\newline
Ahora, como 5 = 5, tenemos 2OE con respecto a la instrucción \textbf{while} $!encontre$, 4OE con respecto a la instrucción \textbf{if} $A[i] = x$ (las dos que teniamos en los casos anteriores pero ahora sumamos 1OE de la asignación $encontre$ $\gets$ $true$ y 1OE del salto del ciclo a la instrucción $print(i-1)$) y 1OE con respecto a la ejecución de $print(i-1)$.
\newline
\newline
Por lo tanto, podemos concluir que la ejecución de  $Buscar(5, [2,6,3,5,8])$ tarda 30 Operaciones Elementales. 
\newline
\newline
\textbf{Reglas generales (pensando en análisis del caso peor)}
\newline
\newline
\textbf{.} El tiempo de ejecución de la sentencia \TipoVariable{CASE C OF $v_{1}$:$S_{1}$|$v_{2}$:$S_{2}$|...|$v_{n}$:$S_{n}$ END} es \TipoVariable{T = T(C) + max\{T($S_{1}$),T($S_{2}$),...,T($S_{n}$)\}}. Observemos que T(C) incluye el tiempo de comparación con $v_{1}$,$v_{2}$,...,$v_{n}$.
\newline
\newline
\textbf{.} El tiempo de ejecución de la sentencia \TipoVariable{IF C THEN $S_{1}$ ELSE $S_{2}$ END} es \TipoVariable{T = T(C) + max\{T($S_{1}$),T($S_{2}$)\}}
\newline
\newline
\textbf{.} El tiempo de ejecución de un bucle de sentencias \TipoVariable{WHILE C DO S END} es \TipoVariable{T = T(C) + (nº iteraciones) * (T(S) + T(C))}. Observemos que tanto T(C) como T(S) pueden variar en cada iteración, y por lo tanto, habrá que tenerlo en cuenta para su cálculo.   
\newline
\newline
\textbf{.} Para calcular el tiempo de ejecución del resto de las sentencias iterativas (\TipoVariable{FOR}, \TipoVariable{REPEAT}, \TipoVariable{LOOP}) basta expresarlas como un bucle \TipoVariable{WHILE}.
\newline
\newline
\textbf{.} El tiempo de ejecución de una llamada a un procedimiento o función \TipoVariable{F($P_{1}$,$P_{2}$,...,$P_{n}$)} es 1 por llamada más el tiempo de evaluación de los parámetros $P_{1}$,$P_{2}$,...,$P_{n}$ más el tiempo que tarde en ejecutarse \TipoVariable{F}, esto es \TipoVariable{T = 1 + T($P_{1}$) + T($P_{2}$) + ... + T($P_{n}$) + T(F)}. No contabilizamos la copia de los argumentos a la pila de ejecución, salvo que se trate de estructuras complejas (registros o vectores) que se pasan por valor. En este caso, contabilizaremos tantas OE como valores simples contenga la estructura. El paso de parámetros por referencia, por tratarse simplemente de punteros, no contabiliza tampoco. 

\subsubsection{Tamaño de la entrada}

Queremos una complejidad relativa, no absoluta. El tamaño de la entrada es una medida general de lo que podemos encontrarnos al ejecutar (queremos predecir, no nos interesa cuanto tarda para una instancia particular sino para clases de instancias).
\newline
\newline
Entonces, presentamos la notación que vamos a utilizar:
\newline
\newline
\textbf{.} \textbf{\textit{T(n)}}: complejidad temporal (o en tiempo) para una entrada de tamaño \textit{n}.
\newline
\newline
\textbf{.} \textbf{\textit{S(n)}}: complejidad espacial para una entrada de tamaño \textit{n}.
\newline
\newline
Entonces, como vamos a medir en función del tamaño del input, tenemos que considerar distintas instancias porque, aunque tengan el mismo tamaño, puede hacer que el algoritmo se comporte de maneras muy diferentes, y por lo tanto, tomar distinto tiempo, y/o requerir distinta cantidad de memoria.
\newline
\newline
Así suelen estudiarse tres casos para un mismo algoritmo: \textbf{caso peor}, \textbf{caso mejor} y \textbf{caso medio}.
\newline
\newline
\textbf{Análisis del caso peor}
\newline
\newline
Sea $t(i)$ el tiempo de ejecución de un algoritmo sobre una instancia $i$.
\newline
\newline
$T_{peor}(n)$ = $max_{instancias~i,~|i|=n}$$\{t(i)\}$
\newline
\newline
Intuitivamente, $T_{peor}(n)$ es el tiempo de ejecución del algoritmo sobre la instancia que implica mayor tiempo de ejecución (entre los inputs de tamaño $n$).
\newline
\newline
Este análisis da \textbf{garantías} sobre las prestaciones del algoritmo.
\newline
\newline
\textbf{Análisis del caso mejor}
\newline
\newline
$T_{mejor}(n)$ = $min_{instancias~i,~|i|=n}$$\{t(i)\}$
\newline
\newline
Intuitivamente, $T_{mejor}(n)$ es el tiempo de ejecución del algoritmo sobre la instancia que implica menor tiempo de ejecución (entre los inputs de tamaño $n$).
\newline
\newline
No da mucha información.
\newline
\newline
\textbf{Análisis del caso medio o promedio}
\newline
\newline
Intuitivamente, $T_{prom}(n)$ corresponde al tiempo \textit{promedio} de ejecución, al tiempo \textit{esperado} sobre instancias \textit{típicas}.
\newline
\newline
Se define como la esperanza matemática de la variable aleatoria definida por todas las posibles ejecuciones del algoritmo para un tamaño de la entrada dado, con las probabilidades de que éstas ocurran para esa entrada.
\newline
\newline
Sea $P(i)$ la probabilidad de que el input sea la instancia $i$.
\newline
\newline
Por lo tanto, $T_{prom}(n)$ = $\displaystyle \sum_{instancias~i,~|i|=n}^{}$$\{P(i)~t(i)\}$
\newline
\newline
Volvamos al ejemplo de búsqueda secuencial.
\newline
\begin{algorithm}[H]{i\tadNombre{busquedaSecuencial}(\In{A}{array(T)}, \In{x}{T})}
	\begin{algorithmic}[1]
		\State $i$ $\gets$ 1
            \State $encontre$ $\gets$ $false$
            \While{$!encontre$}
		\If{$A[i] = x$}
		\State $encontre$ $\gets$ $true$                 
		\State $i$ $\gets$ $i+1$         \EndIf
            \EndWhile
            \State $print(i-1)$ 
		
		\medskip
	\end{algorithmic}
\end{algorithm}
\medskip
Entonces, ahora queremos calcular $T_{peor}(n)$, $T_{mejor}(n)$ y $T_{prom}(n)$.
\newline
\newline
En el peor caso, podemos decir que el elemento se va a encontrar al final del arreglo. Por lo tanto, para las $n-1$ iteraciones anteriores tenemos 5 instruccciones en cada iteración y en el caso de la iteracion $n$ donde se encuentra el elemento buscado, tenemos 6 instrucciones. Ademas, de las 2 instrucciones correspondientes a las primeras dos lineas del algoritmo. Por lo tanto, tenemos que $T_{peor}(n)$ = $2 + 5(n-1) + 6$ = $8 + 5(n-1)$
\newline
\newline
\newline
En el mejor caso, podemos decir que el elemento se va a encontrar en la primera posición del arreglo. Por lo tanto, tenemos que $T_{mejor}(n)$ = $9$
\newline
\newline
En el caso promedio, tendriamos que suponer que alguna hipotesis sobre como está distribuido el input. Es decir, que probabilidad hay de que el elemento buscado esté en la primer posición, que probabilidad hay de que el elemento buscado esté en la segunda posición, que probabilidad hay de que el elemento buscado esté en la tercer posición, etc. Si suponemos que es equiprobable que esté en cualquier posición, vamos a tener un costo para las primeras operaciones y un valor esperado que se encuentre en la posición $\displaystyle \frac{n}{2}$ porque todas las posiciones son equiprobables. Por lo tanto, tenemos aproximadamente que $T_{prom}(n)$ = $8 + 5$ $\displaystyle \frac{n}{2}$
\newline
\newline
¿Cuánto tarda la búsqueda secuencial si el arreglo está ordenado?
\newline
\newline
El algoritmo de búsqueda secuencial tarda lo mismo si el arreglo está ordenado en los tres análisis.
\newline
\newline
¿Cuánto tarda la búsqueda binaria?
\newline
\newline
En este caso, si se producen diferencias porque el ciclo de la búsqueda binaria se ejecuta una cantidad de veces que es logaritmica en $n$ y no una cantidad de veces que es lineal en $n$ como en la búsqueda secuencial.
\newline
\newline
Ahora, para calcular el costo espacial, necesitamos 3 espacios de memorias para almacenar las variables $i$, $encontre$ y la variable $x$ correspondiente al elemento que estamos buscando. Además, necesitamos tambien $n$ espacios en memoria correspondiente al tamaño del arreglo ($n$). Por lo tanto, $S(n)$ = $n+3$. 

\subsubsection{Principio de invarianza}

Dado un algoritmo y dos máquinas (o dos implementaciones) $M_{1}$ y $M_{2}$ que tardan $T_{1}(n)$ y $T_{2}(n)$ respectivamente sobre inputs de tamaño $n$, existe una constante real $c > 0$ y un $n_{0} \in \mathbb{N}$ tales que $\forall$ $n \geq n_{0}$ se verifica que $T_{1}(n)$ $\leq$ $c$ . $T_{2}(n)$  
\newline
\newline
Es decir, que dos ejecuciones distintas del mismo algoritmo sólo difieren en cuanto a eficiencia en un factor constante para grandes valores de la entradas suficientemente grandes.
\newline
\newline
Como \textbf{consecuencia} de esto, no necesitamos usar ninguna unidad para medir el tiempo.

\subsubsection{Análisis asintótico}

Ahora, nos va a interesar calcular, de forma aproximada, el \textbf{orden de magnitud} que tiene el \textbf{tiempo de ejecución} de cada algoritmo.
\newline
\newline
Cuando el tamaño de los datos es pequeño no habrá diferencias significativas en el uso de los distintos algoritmos. 
\newline
\newline
Cuando el tamaño de los datos es \textbf{grande}, los \textbf{costos} de los diferentes algoritmos si pueden variar de manera significativa. 
\newline
\newline
El \textbf{orden} (logarítmico, lineal, cuadrático, exponencial, etc) de la función \textbf{\textit{T(n)}} que mide la complejidad temporal de un algoritmo, es el que \textit{expresa el comportamiento dominante cuando el tamaño de la entrada es grande}. 
\newline
\newline
Es decir, el \textbf{comportamiento asintótico}.
\newline
\newline
El objetivo del estudio de la complejidad algorítmica es determinar el comportamiento asintótico de un algoritmo.
\newline
\newline
Para ello, tenemos distintas medidas del comportamiento asintótico de la complejidad:
\newline
\newline
\textbf{.} $O$ ($O$ grande) cota superior. 
\newline
\newline
\textbf{.} $\Omega$ (omega) cota inferior. 
\newline
\newline
\textbf{.} $\Theta$ (theta) orden exacto de la función. 
\newpage
\subsection{Cota superior. Notación \textit{O}}

La notación \textit{O} sirve para representar el límite o cota superior del tiempo de ejecución de un algoritmo.
\newline
\newline
Más precisamente, la notación \textit{f $\in$ O(g)} expresa que la función \textbf{\textit{f} no crece más rapido que alguna función proporcional a \textit{g}}.
\newline
\newline
En este caso, a \textit{g} se la llama cota superior de \textit{f}.
\newline
\newline
Si para un algoritmo sabemos que $T_{peor} \in O(g)$ se puede asegurar que para inputs de tamaño creciente, \textbf{en todos los casos} el tiempo será a lo sumo proporcional a la cota.
\newline
\newline
Si para un algoritmo sabemos que $T_{prom} \in O(g)$ se puede asegurar que para inputs de tamaño creciente, \textbf{en promedio} el tiempo será a lo sumo proporcional a la cota.
\newline
\newline
Veamos más formalmente la notación \textit{O}.
\newline
\newline
Asumiendo funciones reales no negativas con dominio en los naturales:
\newline
\newline
$f \in O(g)$ significa que \textit{f} no crece más que \textit{g}.  
\newline
\newline
$O(g)$ = \{$f$ $|$ $\exists$ $n_{0}$, $k > 0$ tal que $n \geq n_{0}$ $\Rightarrow$ $f(n)$ $\leq$ $k$ · $g(n)$\} 

\subsubsection{Propiedades de \textit{O}}

A continuación, mencionamos algunas propiedades de la notación \textit{O}:
\newline
\newline
1. Para cualquier función $f$ se tiene que $f \in O(f)$.
\newline
\newline
2. $f \in O(f)$ $\Rightarrow$ $O(f)$ $\subset$ $O(g)$. 
\newline
\newline
3. $O(f)$ $=$ $O(g)$ $\Leftrightarrow$ $f \in O(g)$ y $g \in O(f)$
\newline
\newline
4. Si $f \in O(g)$ y $g \in O(h)$ $\Rightarrow$ $f \in O(h)$.
\newline
\newline
5. Si $f \in O(g)$ y $f \in O(h)$ $\Rightarrow$ $f \in O(min(g,h))$. 
\newline
\newline
6. Regla de la suma: Si $f_{1} \in O(g)$ y $f_{2} \in O(h)$ $\Rightarrow$ $f_{1} + f_{2} \in O(max(g,h))$   
\newline
\newline
7. Regla del producto: Si $f_{1} \in O(g)$ y $f_{2} \in O(h)$ $\Rightarrow$ $f_{1} * f_{2} \in O(g * h)$ 
\newline
\newline
8. Si existe $\displaystyle \lim_{n \to \infty}$ $\displaystyle \frac{f(n)}{g(n)}$ $=$ $k$, según los valores que tome $k$:
\newline
\newline
a) Si $k \neq 0$ y $k < \infty$, entonces $O(f) = O(g)$  
\newline
\newline
b) Si $k = 0$, entonces $f \in O(g)$, es decir, $O(f) \subset O(g)$ pero sin embargo se verifica que $g \notin O(f)$. 
\newpage 

\subsubsection{Funciones de complejidad temporal}

Veamos algunas funciones de complejidad temporal. 
\newline
\newline
\textit{O(1)} \textbf{Complejidad constante}: Es independiente de los datos de entrada.
\newline
\newline
\textit{O(lg(n))} \textbf{Complejidad logarítmica}: Suele aparecer en determinados algoritmos con iteración o recursión (por ejemplo, búsqueda binaria). Todos los logaritmos, sea cual sea su base, son del mismo orden, por lo que se representan en cualquier base.
\newline
\newline
\textit{O(n)} \textbf{Complejidad lineal}: Suele aparecer en bucles simples cuando la complejidad de las operaciones internas es constante o en algunos algoritmos con recursión.
\newline
\newline
\textit{O(n lg(n))} En algunos algoritmos Divide \& Conquer (por ejemplo, Mergesort).
\newline
\newline
\textit{O($n^{2}$)} \textbf{Complejidad cuadrática}: Aparece en bucles o recursiones doblemente anidados.
\newline
\newline
\textit{O($n^{3}$)} \textbf{Complejidad cúbica}: En bucles o recursiones triples.
\newline
\newline
\textit{O($n^{k}$)} \textbf{Complejidad polinómica} ($k \geq 1$)
\newline
\newline
\textit{O($2^{n}$)} \textbf{Complejidad exponencial}: Suele aparecer en subprogramas recursivos que contengan dos o más llamadas internas.

\subsection{Cota inferior. Notación $\Omega$}

La notación $\Omega$ sirve para representar el límite o cota inferior del tiempo de ejecución de un algoritmo.
\newline
\newline
Más precisamente, la notación \textit{f $\in$ $\Omega(g)$} expresa que la función \textbf{\textit{f} está acotada inferiormente por alguna función proporcional a \textit{g}}.
\newline
\newline
En este caso, a \textit{g} se la llama cota inferior de \textit{f}.
\newline
\newline
Si para un algoritmo sabemos que $T_{peor} \in \Omega(g)$ se puede asegurar que para inputs de tamaño creciente, el tiempo será, en el peor caso, al menos proporcional a la cota.
\newline
\newline
La notación se usa tambien para dar cotas inferiores para problemas. A veces se puede decir para un problema que \textbf{para cualquier algoritmo que lo resuelva}, $T_{peor} \in \Omega(g)$, lo que significa que cualquier algoritmo que lo resuelva tiene una complejidad, en el peor caso, proporcional a la cota.
\newline
\newline
Veamos más formalmente la notación $\Omega$.
\newline
\newline
$f \in \Omega(g)$ significa que \textit{f} crece al menos como \textit{g}.  
\newline
\newline
$\Omega(g)$ = \{$f$ $|$ $\exists$ $n_{0}$, $k > 0$ tal que $n \geq n_{0}$ $\Rightarrow$ $f(n)$ $\geq$ $k$ · $g(n)$\} 

\subsubsection{Propiedades de $\Omega$}

A continuación, mencionamos algunas propiedades de la notación $\Omega$:
\newline
\newline
1. Para cualquier función $f$ se tiene que $f \in \Omega(f)$.
\newline
\newline
2. $f \in \Omega(f)$ $\Rightarrow$ $\Omega(f)$ $\subset$ $\Omega(g)$. 
\newline
\newline
3. $\Omega(f)$ $=$ $\Omega(g)$ $\Leftrightarrow$ $f \in \Omega(g)$ y $g \in \Omega(f)$
\newline
\newline
4. Si $f \in \Omega(g)$ y $g \in \Omega(h)$ $\Rightarrow$ $f \in \Omega(h)$.
\newline
\newline
5. Si $f \in \Omega(g)$ y $f \in \Omega(h)$ $\Rightarrow$ $f \in \Omega(max(g,h))$. 
\newline
\newline
6. Regla de la suma: Si $f_{1} \in \Omega(g)$ y $f_{2} \in \Omega(h)$ $\Rightarrow$ $f_{1} + f_{2} \in \Omega(g + h)$   
\newline
\newline
7. Regla del producto: Si $f_{1} \in \Omega(g)$ y $f_{2} \in \Omega(h)$ $\Rightarrow$ $f_{1} * f_{2} \in \Omega(g * h)$ 
\newline
\newline
8. Si existe $\displaystyle \lim_{n \to \infty}$ $\displaystyle \frac{f(n)}{g(n)}$ $=$ $k$, según los valores que tome $k$:
\newline
\newline
a) Si $k \neq 0$ y $k < \infty$, entonces $\Omega(f) = \Omega(g)$  
\newline
\newline
b) Si $k = 0$, entonces $g \in \Omega(f)$, es decir, $\Omega(g) \subset \Omega(f)$ pero sin embargo se verifica que $g \notin O(f)$. 

\subsection{Orden exacto. Notación $\Theta$}

Como última cota asintótica, definiremos los conjuntos de funciones que crecen asintóticamente de la misma forma
\newline
\newline
$\Theta(f)$ = $O(f)$ $\cap$ $\Omega(f)$
\newline
\newline
Intuitivamente, $t \in \Theta(f)$ indica que $t$ está acotada por $f$ tanto superior como inferiormente. 
\newline
\newline
Más precisamente, la notación \textit{f $\in$ $\Theta(g)$} expresa que la función \textit{f} crece (a partir de cierto momento) igual que \textit{g}.
\newline
\newline
Veamos más formalmente la notación $\Theta$.
\newline
\newline
$\Theta(g)$ = \{$f$ $|$ $\exists$ $n_{0}$, $k_{1},k_{2} > 0$ tal que $n \geq n_{0}$ $\Rightarrow$ $k_{1}$ · $g(n)$ $\leq$ $f(n)$ $\leq$ $k_{2}$ · $g(n)$\} 

\subsubsection{Propiedades de $\Theta$}

A continuación, mencionamos algunas propiedades de la notación $\Theta$:
\newline
\newline
1. Para cualquier función $f$ se tiene que $f \in \Theta(f)$.
\newline
\newline
2. $f \in \Theta(g)$ $\Rightarrow$ $\Theta(f)$ $=$ $\Theta(g)$. 
\newline
\newline
3. $\Theta(f)$ $=$ $\Theta(g)$ $\Leftrightarrow$ $f \in \Theta(g)$ y $g \in \Theta(f)$
\newline
\newline
4. Si $f \in \Theta(g)$ y $g \in \Theta(h)$ $\Rightarrow$ $f \in \Theta(h)$.
\newline
\newline
5. Si $f \in \Theta(g)$ y $f \in \Theta(h)$ $\Rightarrow$ $f \in \Theta(max(g,h))$. 
\newline
\newline
6. Regla de la suma: Si $f_{1} \in \Theta(g)$ y $f_{2} \in \Theta(h)$ $\Rightarrow$ $f_{1} + f_{2} \in \Theta(max(g + h))$   
\newline
\newline
7. Regla del producto: Si $f_{1} \in g + \Theta(g)$ y $f_{2} \in \Theta(h)$ $\Rightarrow$ $f_{1} * f_{2} \in \Theta(g * h)$ 
\newline
\newline
8. Si existe $\displaystyle \lim_{n \to \infty}$ $\displaystyle \frac{f(n)}{g(n)}$ $=$ $k$, según los valores que tome $k$:
\newline
\newline
a) Si $k \neq 0$ y $k < \infty$, entonces $\Theta(f) = \Theta(g)$  
\newline
\newline
b) Si $k = 0$, entonces $\Theta(g) \neq \Theta(f)$ porque ambas funciones no están creciendo al mismo ritmo o tasa.
\newpage

\section{Clase Teórica 04. Diseño de Tipos Abstractos de Datos}

Ahora, nos vamos a enfocar en el diseño jerárquico de TADs.
\newline
\newline
Entonces, ¿Qué significa diseñar?
\newline
\newline
Diseñar significa pasar de la descripción del \textbf{qué} del problema al \textbf{cómo}.
\newline
\newline
Queremos permitir un cambio de paradigma (del utilizado para especificar al utilizado para programar) que resulte ordenado, metódico, mas o menos formal.
\newline
\newline
\textbf{Ejemplo}
\newline
\newline
Consideremos el TAD \tadNombre{Conjunto}.
\newline
\newline
Veamos dos implementaciones posibles.
\newline
\newline
1. Un arreglo dimensionable donde:
\newline
\newline
\textbf{.} La inserción (sin repetidos) tiene un costo de $O(n)$ donde los elementos se colocan en su posición correspondiente, es decir, luego de la inserción, quedan ordenados.
\newline
\newline
\textbf{.} La búsqueda tiene un costo de $O(log(n))$ dado que se realiza aplicando búsqueda binaria.
\newline
\newline
2. Una secuencia donde:
\newline
\newline
\textbf{.} La inserción (sin repetidos) tiene un costo de $O(1)$ porque los elementos se colocan al final o al principio de la secuencia.
\newline
\newline
\textbf{.} La búsqueda tiene un costo de $O(n)$ dado que en el peor caso habrá que recorrer toda la secuencia.
\newline
\newline
¿Cuál estructura me conviene?
\newline
\newline
Depende del contexto de uso en el cual este usando la estructura de datos.
\newline
\newline
\textbf{La etapa de diseño} es una capa intermedia que nos va a permitir pasar de la especificación a la implementación del código.
\newline
\newline
Por lo tanto, en la etapa de diseño debemos:
\newline
\newline
\textbf{.} Proveer una \textbf{representación} para los valores
\newline
\newline
\textbf{.} Definir las \textbf{funciones} del tipo, es decir, proveer algoritmos para implementar las funciones del tipo. 
\newline
\newline
\textbf{.} Demostrar que eso es \textbf{correcto}.
\newline
\newline
¿Qué significa jerárquico?
\newline
\newline
Que pensaremos la resolución del \textit{cómo} a partir de representaciones de un tipo sobre otros separando responsabilidades en la construcción de la solución. 

\subsection{Contexto de uso}

¿Cómo discriminamos entre dos soluciones?
\newline
\newline
De acuerdo al contexto de uso, y los requerimientos de eficiencia.

\subsection{Metodología de diseño}

Desde un punto de vista abstracto, diseñar implica las siguientes tareas:
\newline
\newline
\textbf{.} Elección del TAD a diseñar siempre siguiendo una filosofía \textit{top-down}, es decir, comenzando siempre por el TAD más abarcativo.
\newline
\newline
\textbf{.} Introducción de los elementos no funcionales
\newline
\newline
\textbf{.} Vinculación entre la representación y su abstracción
\newline
\newline
\textbf{.} Iteración sobre los tipos restantes (filosofia \textit{top-down}).
\newline
\newline
Dentro de esta metodología, vamos a tener que enfocarnos en dos aspectos claves:
\newline
\newline
\textbf{.} Los \textbf{aspectos de la interfaz} de un tipo describen todo elemento relacionado con los aspectos de uso de dicho tipo, es decir, toda cuestión referida a lo que resulte visible desde afuera.
\newline
\newline
\textbf{.} Las \textbf{pautas de la implementación} serán todo aspecto que refiera a cuestiones vinculadas a los medios a través de los cuales el tipo garantiza esos aspectos de uso.
\newline
\newline
La definición de la \textbf{Interfaz} de un módulo de diseño implica tomar en cuenta de varias cosas y esencialmente debe explicarle al eventual usuario todos los aspectos relativos a los servicios que exporta:
\newline
\newline
\textbf{Servicios exportados}: desccribe para cada operación su complejidad, aspectos de aliasing, efectos colaterales sobre los argumentos, etc.
\newline
\newline
\textbf{Interfaz}: define en el paradigma imperativo las operaciones exportadas junto con su precondición y postcondición. Esto establecerá la relación entre la implementación de las operaciones y su especificación. 

\subsubsection{Transparencia referencial, aliasing, etc}

Una función es \textbf{referencialmente transparente} si su resultado solo depende de sus parámetros explicitos.
\newline
\newline
Por ejemplo:
\newline
\newline
\textbf{.} Si \textit{f(x) = \{return x+1\}}, $f(4)$  $+$ $f(3)$ es referencialmente transparente.
\newline
\newline
\textbf{.} Si \textit{f(x) = \{y = G*(x+1); G++; return y\}}, no es referencialmente transparente porque no conocemos el valor de la variable global $G$.
\newline
\newline
\textbf{Aliasing} significa la posibilidad de tener más de un nombre para la misma cosa. En concreto, dos punteros o referencias hacia el mismo objeto.
\newline
\newline
Veamos el siguiente ejemplo:
\newline
\newline
Una operación que dado un arbol binario (no vacio) nos devolviera dos árboles y un elemento (subarbol izquierdo, derecho y raiz).
\newline
\newline
\TipoVariable{Podar(in A: ab(elem), out I: ab(elem), out r: elem, out D: ab(elem))}
\newline
\newline
Implementación 1: armar copias de los subarboles izquierdo y derecho de A, devolverlas como I y D.
\newline
\newline
Implementación 2 (más rápida): devolver en I y D referencias a los subárboles de A.
\newline
\newline
En este último caso, estariamos provocando \textit{aliasing} entre los árboles, porque cualquier modificación que se realice sobre I o D, luego de ejecutar la operación Podar, repercutirá en A.
\newpage
\subsection{Ejemplo con Conj(NAT)}

Nos solicitan el siguiente contexto de uso:
\newline
\newline
\textbf{.} La obtención del minimo debe tener complejidad $O(1)$.
\newline
\newline
\textbf{.} Quitar un elemento debe tener complejidad $O(n)$, siendo $n$ la cantidad de elementos agregados.
\newline
\newline
\textbf{.} Agregar un elemento debe tener complejidad $O(1)$.
\newline
\newline
\textbf{Servicios exportados:}
\newline
\newline
\textbf{.} pertenece: No produce aliasing ni efectos colaterales sobre los argumentos, posee orden de complejidad temporal $O(n)$ con $n$ la cantidad de elementos agregados al conjuntos.
\newline
\newline
\textbf{.} vacio: No produce aliasing ni efectos colaterales, posee orden de complejidad temporal $O(1)$. 
\newline
\newline
\textbf{.} agregar: No produce aliasing, modifica colateralmente el conjunto argumento, posee orden de complejidad temporal $O(1)$. 
\newline
\newline
\textbf{.} quitar: No produce aliasing sobre los argumentos, modifica colateralmente el conjunto argumento, posee orden de complejidad temporal $O(n)$ con $n$ la cantidad de elementoss agregados al conjunto.
\newline
\newline
\textbf{.} vacio?: No produce aliasing ni efectos colaterales, posee orden de complejidad temporal $O(1)$.  
\newline
\newline
\textbf{.} mínimo: No produce aliasing ni efectos colaterales, posee orden de complejidad temporal $O(1)$.
\newline
\newline
\textbf{FALTARIA COMPLETAR LA INTERFAZ!!!}
\newpage

\subsection{La Representación}

La definición de la \textbf{representación} de un módulo de diseño implica tomar cuenta todo aspecto referido a cómo se satisfacen los requerimientos declarados en la interfaz.
\newline
\newline
\textbf{. Estructura}: Describe la estructura interna sobre la cual las operaciones aplican.
\newline
\newline
\textbf{. Relación entre la representación y la abstracción}: Por un lado, expone toda restricción sobre la estructura de representación a fin de que efectivamente pueda ser considerada una implementación de un valor del tipo al que implementa; y por otro lado, vincula los valores con su contraparte abstracta, es decir, con algún término de la especificación a quien este represente.
\newline
\newline
\textbf{. Algoritmos}: Mostrar como se realizan los algoritmos para implementar las funciones correspondientes al TAD incluyendo tanto las operaciones exportadas como las auxiliares junto con su cálculo detallado que justifica su complejidad. 
\newline
\newline
\textbf{. Servicios usados:} Declara toda demanda de complejidad, aliasing o efecto colateral que los servicios usados de otros tipos en la programación de los algoritmos deban satisfacer. 

\subsubsection{Estructura de representación}

La estructura de representación describe los valores sobre los cuales se representará el género que se está implementando.
\newline
\newline
Veamos esto con un ejemplo.
\newline
\newline
Queremos implementar un conjunto de naturales con el siguiente contexto: los números del 1 al 100 deben manejarse en $O(1)$ porque se usan mucho. El resto, en $O(n)$. Rápidamente debo conocer la cardinalidad.
\newline
\newline
Proponemos la siguiente estructura:
\newline
\newline
\textbf{.} Un arreglo de 100 posiciones booleanas donde haya 1 en la posición $i$ si el elemento $i$ pertenece al conjunto. De esta forma, con acceder a la posición correspondiente al número, se verifica si pertenece o no al conjunto. 
\newline
\newline
\textbf{.} Una secuencia que incluye todos los elementos mayores a 100.
\newline
\newline
\textbf{.} Un nat para la cardinalidad.
\newline
\newline
Luego, escribimos esta estructura de representación como:
\begin{Estructura}{conj\_semi\_rapido}[estr]
	\begin{Tupla}[estr]%
		\tupItem{rapido}{arreglo [1,...,100] de nat}%
		\tupItem{resto}{secu(nat)}%
		\tupItem{cardinal}{nat}%
	\end{Tupla}
\end{Estructura}
\medskip
\medskip
\subsubsection{El invariante de representación}

¿Cualquier instancia es válida?
\newline
\newline
¿$\langle$[0,...,0], $\secuvacia$, 8254$\rangle$ es un \TipoVariable{conj\_semi\_rapido} válido? No, porque 8254 representa la cardinal del conjunto y como el arreglo correspondiente a los numeros del 1 al 100 se encuentra completo de 0s, su cardinalidad es 0 y los números mayores a 100 están representados por una secuencia vacia, no hay ningún número mayor a 100 que pertenezca al conjunto. Por lo tanto, la cardinalidad del conjunto es 0 + 0 = 0 != 8254.
\newline
\newline
¿$\langle$[0,...,0], $\langle$37,107,28$\rangle$, 3$\rangle$ es un \TipoVariable{conj\_semi\_rapido} válido? No, porque a pesar de que el arreglo de número del 1 a 100 tiene cardinalidad 0 porque está lleno de 0s y la secuencia de números mayores a 100 tiene 3 elementos tanto el 37 como el 28 no son mayores a 100.
\newpage
¿Para que nos sirve separar con facilidad instancias válidas de invalidas?
\newline
\newline
\textbf{.} Como una forma de documentar la estructura.
\newline
\newline
\textbf{.} Como condición necesaria para establecer una relación con la abstracción.
\newline
\newline
\textbf{.} Para agregar a las postcondiciones, como una forma de garantizar que nuestros algoritmo no rompen la estructura.
\newline
\newline
\textbf{.} Para agregar a las precondiciones, como una garantia con la que cuentan nuestros algoritmos.
\newline
\newline
\textbf{.} Como una guia a la hora de escribir los algoritmos porque deben valer tanto en la precondición como en la postcondición
\newline
\newline
\textbf{.} Si pudiésemos programar el chequeo, como una forma adicional de detectar instancias corruptas.
\newline
\newline
El \textbf{invariante de representación} es una función booleana con dominio en el género de representación que da $true$ cuando recibe una instancia válida.
\newline
\newline
Si representamos $T_{1}$ sobre $T_{2}$:
\newline
\Rep[$T_{2}$][t]{... condiciones que garanticen que $t$ representa una instancia válida de $T_{1}$...}
\medskip
\medskip
Volviendo al ejemplo, las condiciones del invariante de representación, descriptas de manera informal, deberían ser las siguientes:
\newline
\newline
1) Que $resto$ solo tenga números mayores que 100.
\newline
\newline
2) Que $resto$ no tenga números repetidos.
\newline
\newline
3) Que $cardinal$ tenga la longitud de resto más la cantidad de celdas de \textit{rápido} que esten en 1 ($true$).
\newline
\newline
Veamos la descripcion formal del invariante de representación: 
\newline
\Rep[estr][e]{(1) ($\forall n$:nat) (está?($n,e.resto$) $\impluego$ $n > 100$) $\yluego$ (2) ($\forall n$:nat) (cant\_apariciones($n,e.resto$) $\leq$ $1$) $\yluego$ \newline (3) $e.cant$ $=$ $long(e.resto)$ $+$ cant\_trues($e.rapido$)}
\medskip
\medskip
\medskip
donde \textit{está}, $cant\_de\_apariciones$ y $long$ son funciones de secuencias, y $contar\_trues$ es una función sobre arreglos. En los problemas más complicados se vuelve muy importante analizar metódicamente la estructura de representación a la hora de elegir los predicados del invariante. Una forma de organizarse es empezar mirando cada campo de la estructura de forma individual, para ver bien que condiciones tienen que cumplir. Una vez analizado esto, podemos ver la relación que deben tener los distintos campos entre si para que su información sea consistente.
\newline
\newline
\textbf{Función de abstracción}
\newline
\newline
La función de abstracción es una herramienta que permite vincular una estructura con \textbf{algún} valor abstracto al que representa ¿Cómo les parece que esto puede hacerse? Es decir, ¿Qué debemos usar para caracterizar algún término abstracto que representa una estructura particular?
\newline
\newline
$Abs$: $T_{2}$ $e$ $\rightarrow$ $T_{1}$ (Rep($e$))
\newline
\newline
Por lo tanto, la función de abstracción tiene como dominio al conjunto de instancias que son la imagen abstracta del tipo al que representa (y verifican el invariante de representación) y devuelve una imagen abstracta de la instancia del tipo representado, es decir, aquella instancia que queremos representar.
\newline
\newline
La manera en la que se caracteriza un término es o bien a través de los generadores o de los observadores. Normalmente el uso de los observadores resulta más sencillo.
\newpage
Anteriormente, habiamos afirmado que \TipoVariable{conjuntoDeNat} \textbf{se representa con} \TipoVariable{$\langle$secuenciaDeNat, nat$\rangle$} 
\newline
\newline
Ahora, la función de abstracción viene a cumplir la vuelta de esto, es decir, si tengo \TipoVariable{$\langle$secu[nat], nat$\rangle$} y le aplico la función de abstracción, vuelvo al \TipoVariable{conj[nat]}, es decir, \TipoVariable{$\langle$secu[nat], nat$\rangle$} \textbf{representa a (abs)} al  \TipoVariable{conj[nat]}.
\newline
\newline
Volviendo al ejemplo del \TipoVariable{conjunto\_semi\_rapido}, esccribimos su función de abstracción:
\newline
\AbsFc[estr]{conjunto\_semi\_rapido $C$}[e]{ $C$ / ($\forall n$:nat) ($n$ $\in$ $C$ $\impluego$ ($n \geq 100$ $\yluego$ $r.rapido[n]$) $\lor$ ($n > 100$ $\yluego$ esta?($n,e.resto$)))}
\medskip
\medskip
\textbf{Propiedades}
\newline
\newline
\textbf{.} Una vez restringida a ($Rep(e)$), la función de abstracción debe ser total.
\newline
\newline
\textbf{.} No tiene por qué ser inyectiva. Dos estructuras diferentes pueden representar al mismo término de un TAD. Veamos el siguiente ejemplo:
\newline
\newline
\TipoVariable{Abs: secu(nat) $s$ $\rightarrow$ conj(nat)}
\newline
\newline
\TipoVariable{Abs(1 $\puntito$ 2 $\puntito$ 3 $\puntito$ $\secuvacia$)} $\igobs$ \{1,2,3\} $\igobs$ \TipoVariable{Abs(3 $\puntito$ 1 $\puntito$ 2 $\puntito$ $\secuvacia$)}
\newline
\newline
\textbf{.} Debe ser suryectiva sobre las clases de equivalencia determinadas por la igualdad observacional, restringidas a lo especificado por el contexto de uso.
\newline
\newline
\textbf{.} No tiene por qué ser suryectiva sobre todos los términos del género representado. Es decir, no podemos asegurar que todo término del TAD va a ser imagen de la función de abstracción para la estructura de representación que estemos utilizando.  

\subsubsection{Los algoritmos}

Recordemos la interfaz de \tadNombre{Agregar(...)}:
\newline
\newline
\InterfazFuncion{Agregar}{\Inout{C}{conjunto\_semi\_rapido}, \In{e}{nat}}{}%
[$C$ $\igobs$ $C_{0}$ $\land$ $e \notin C$]
{$C$ $\igobs$ $Ag(C_{0},e)$}%
\newline
\newline
Luego, una posible implementacion del algoritmo podria ser la siguiente:

\begin{algorithm}[H]{i\tadNombre{Agregar}(\Inout{C}{estr}, \In{e}{nat})}
	\begin{algorithmic}[1]
		\State $C.cant++$                      
		\If{$e < 100$}
		\State $C.rapido[e]$ $=$ $true$                      
		\Else
		\State AgregarAtras($C.resto,e$)                     
		\EndIf
		
		\medskip
	\end{algorithmic}
\end{algorithm}
\newpage
\textbf{Probando corrección}
\newline
\newline
Veamos una funcionalidad adicional que tiene la función de abstracción.
\newline
\newline
Veamoslo con un ejemplo para Conjunto implementado sobre secuencia.
\newline
\newline
Dada una secuencia $s$, el resultado de la operación agregar un natural $n$ sobre la abstracción de la secuencia $Ag(Abs(s),n)$ debe ser observacionalmente equivalente a agregar el natural a la abstracción de esa agregación, es decir, $Ag(Abs(s),n)$ $\igobs$ $Abs(s \circulito n)$
\newline
\newline
De forma más general, queremos decir que la función de abstracción debe ser un homomorfismo respecto de la signatura del TAD, o sea, que para toda operación $\puntito$, $Abs(i \puntito (p_{1},...,p_{n}))$ $\igobs$ $\puntito$($Abs(p_{1})$,...,$Abs(p_{n})$) 
\newline
\newline
Es decir, que la abstracción de la implementación de una operación $\puntito$ tiene que ser observacionalmente equivalente a la implementación de la abstracción de esa operación.  
\newpage

\section{Clase Teórica 05. Diseño de Conjuntos y Diccionarios}

Ahora, nos vamos a enfocar en soluciones a problemas concretos que nos van a permitir avanzar en la implementación de la solución para una serie de problemas fundamentales para la informatica. En particular, vamos a trabajar con \textit{conjuntos} y \textit{diccionarios}.  
\newline
\newline
Presentemos, entonces, el TAD \tadNombre{Diccionario}.
\newline
\begin{tad}{\tadNombre{Diccionario(clave, significado)}}
	\medskip	
        \textbf{observadores basicos}
        \medskip
	\tadOperacion{def?}{clave/c, dicc(clave;significado)/d}{bool}{}
 \medskip
	\tadOperacion{obtener}{clave/c, dicc(clave;significado)/d}{significado}{def?($c,d$)}
	\medskip
        \medskip
        \textbf{generadores}
	\medskip
	\tadOperacion{vacio}{}{dicc(clave,significado)}{}
	\medskip
	\tadOperacion{definir}{clave, significado, dicc(clave;significado)}{dicc(clave,significado)}{}
        \medskip
        \medskip
        \textbf{otras operaciones}
        \medskip
\tadOperacion{borrar}{clave/c, dicc(clave;significado)/d}{dicc(clave,significado)}{def?($c,d$)}
	\medskip
 \tadOperacion{claves}{dicc(clave;significado)}{conj(clave)}{}
	\medskip
 \tadOperacion{. =dicc}{dicc($\alpha$), dicc($\alpha$)}{bool}{}
	\medskip
\end{tad}
\medskip
\medskip
A partir de ahora, vamos a buscar representaciones o implementaciones eficientes de conjuntos y diccionarios.

\subsection{Representación secuencial de conjuntos y diccionarios}

Los conjuntos y diccionarios pueden representarse a través de estructuras secuenciales.
\newline
\newline
Consideramos para ello la siguiente estructura:

\begin{Estructura}{diccionario}[estr]
	\begin{Tupla}[estr]%
		\tupItem{dicc}{[tupla(clave,significado)]}%
		\tupItem{ult}{puntero(tupla(clave,significado))}%
	\end{Tupla}
\end{Estructura}
\medskip
En esta estructura, $dicc$ es un arreglo de tuplas donde en cada posición se encuentra una tupla donde en la primer componente se encuentra definida la $clave$ y en la segunda su correspondiente $significado$. Luego, $ult$ es un puntero que apunta a la ultima tupla insertada en el arreglo.
\newline
\newline
Ahora, calculemos el orden de complejidad utilizando la estructura de las operaciones básicas de un diccionario.
\newline
\newline
La operación \textbf{vacio} tiene costo $O(1)$.
\newline
\newline
La operación \textbf{definir} tiene costo $O(1)$ dado que tenemos que mover el puntero una posición más de la que se encuentra actualmente e insertar alli la tupla con la clave y su significado.
\newline
\newline
La operación \textbf{def?} tiene costo $O(n)$ porque debemos buscar de manera secuencial por todo el arreglo para chequear si la clave se encuentra definida en alguna tupla suponiendo que $n$ son la cantidad de tuplas clave-significado definidas en el arreglo.
\newline
\newline
La operación \textbf{obtener} tiene costo $O(n)$ porque tenemos costo $O(n)$ para realizar la búsqueda de la clave en el arreglo y una vez encontrada, obtener tiene costo $O(1)$. Por lo tanto, $O(n + 1)$ = $O(max\{n,1\})$ = $O(n)$.
\newline
\newline
Otra estructura que podriamos plantear en la misma que en el caso anterior pero que el arreglo se encuentre \textbf{ordenado}.
\newline
\newline
En este caso, la operación de crear un arreglo vacio tiene costo $O(1)$ pero la operación $definir$ tiene un costo $O(n)$ dado que ahora no se puede mover el puntero a la última posición del arreglo sino que hay que ubicar a la tupla en su posición correspondiente para que quede ordenada y en el peor caso, podriamos querer colocar una tupla donde su clave tenga valor mínimo y esto implicaría mover a todas las tuplas una posición hacia abajo generando un costo de $O(n)$. Mientras que las operaciones $def?$ y $obtener$ pasan a tener costo $O(lg(n))$ porque podemos aplicar búsqueda binaria.
\newline
\newline
Otra estructura que podriamos plantear es una lista de clave significado \TipoVariable{lista(tupla(clave,significado))} donde las operaciones tienen el mismo costo que en el caso del arreglo no ordenado.
\newline
\newline
Otra estructura que podriamos plantear es una lista de clave significado \TipoVariable{lista(tupla(clave,significado))} que se encuentre \textbf{ordenada} pero sería el caso menos beneficioso porque crear un diccionario vacio tiene costo $O(1)$, la operación $definir$ tiene costo $O(n)$ pero las operaciones $def?$ y $obtener$ tienen costo $O(n)$ porque en listas no se puede realizar búsqueda binaria.
\newline
\newline
Ahora, la complejidad de las operaciones tienen costo:
\newline
\newline
\textbf{.} Tiempo: $O(n)$ en el peor caso en al menos alguna operación.
\newline
\newline
\textbf{.} Espacio: $O(n)$
\newline
\newline
¿Se podrá hacer mejor?

\subsection{Representación de conjuntos y diccionarios a través de Arboles Binarios}

El costo de crear un diccionario vacio es $O(1)$ porque simplemente debemos crearlo y setearlo a \textit{nil}
\newline
\newline
Para calcular el costo de la operación \textit{def?}, tendría que crear un algoritmo para recorrer cada uno de los nodos del árbol e ir chequeando si es igual al elemento que estamos buscando. En el peor caso, vamos a recorrer todos los nodos del árbol.
\newline
\newline
Para \textit{definir} o agregar un nuevo elemento al árbol, podría considerar colocarlo como hoja en algún espacio que se encuentre disponible sin necesidad de inaugurar un nuevo nivel. En ese caso, estariamos trabajando con \textit{árboles completos} y tener un puntero al espacio libre o simplemente buscarlo por todo el árbol hasta encontrarlo lo cual produce un costo de $O(n)$ con $n$ la cantidad de elementos del árbol.

\subsection{Árbol Binario de Búsqueda}

Un Árbol Binario de Búsqueda es un árbol binario que satisface la siguiente propiedad:
\newline
\newline
Para todo nodo, los valores de los elementos en su subárbol izquierdo son menores que el valor del nodo, y los valores de los elementos de su subárbol derecho son mayores que el valor del nodo.
\newline
\newline
Dicho de otra forma, el valor de todos los elementos del subárbol izquierdo es menor que el valor de la raíz, el valor de todos los elementos del subárbol derecho es mayor que el valor de la raíz, y tanto el subárbol izquierdo como el subárbol derecho... son ABB 
\newpage
Veamos un ejemplo de ABB.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{abb-1}
	\label{drivers1}
\end{figure}
\medskip
\medskip
Es efectivamente un ABB porque todos los nodos del subárbol izquierdo son menores que la raíz (21) y todos los nodos del subárbol derecho son mayores que la raíz (21). Además, todos los subárboles izquierdos y derechos son ABBs.
\newline
\Rep[$ab(nodo)$][e]{(0) ABB($e$) $=$ Nil?($e$) $\oluego$ \newline
	 (1) ($\forall c$:$clave$) (esta?($c,Izq(e)$) $\impluego$ $c < clave(raiz(e))$) $\land$ \newline 
	 (2) ($\forall c$:$clave$) (esta?($c,Der(e)$) $\impluego$ $c > clave(raiz(e))$) $\land$ \newline 
	 (3) $ABB(Izq(e))$
 	\newline 
 	(4) $ABB(Der(e))$}
\medskip
\medskip
\medskip
Ahora, veamos algunos \textbf{algoritmos} básicos de los ABB:

\begin{algorithm}[H]{i\tadNombre{Vacio}(\In{}{$\secuvacia$}, \Out{A}{ab(nodo)})}
	\begin{algorithmic}[1]
		\State \textbf{return} NIL 
		
		\medskip
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]{i\tadNombre{Definir}(\In{c}{clave}, \In{s}{significado}, \Inout{A}{ab(nodo)})}
	\begin{algorithmic}[1]
		\If{$A == nil$}
		\State $A$ $\gets$ $ab(NIL, \langle c,s \rangle, NIL)$                      
		\Else
		\If{$c < r_{c}$}
		\State $A$ $\gets$ $ab(definir(c,s,I), \langle r_{c},r_{s} \rangle, D)$
		\Else                    
		\State $A$ $\gets$ $ab(I, \langle r_{c},r_{s} \rangle, definir(c,s,D))$                     
		\EndIf
		\EndIf
		
		\medskip
	\end{algorithmic}
\end{algorithm}
\newpage

\subsubsection{Inserción en ABB}

Basicamente, para realizar una inserción en un ABB debemos realizar dos pasos:
\newline
\newline
\textbf{.} Buscar al padre del nodo a insertar 
\newline
\newline
\textbf{.} Insertarlo como hijo de ese padre.
\newline
\newline
Entonces, formemos un ABB con los elementos del arreglo \TipoVariable{[49,21,52,56,67,54,77,83,75]}.
\newline
\newline
\textbf{.} Para insertar el nodo 49, como el árbol está vacio, inserto este nodo como raíz.
\newline
\newline
\textbf{.} El nodo 21, como 21 $<$ 49, lo inserto como hijo izquierdo del 49.
\newline
\newline
\textbf{.} El nodo 52, como 52 $>$ 49, lo inserto como hijo derecho del 49.
\newline
\newline
\textbf{.} El nodo 56, como 56 $>$ 49 y 56 $>$ 52 lo inserto como hijo derecho del 52.
\newline
\newline
\textbf{.} El nodo 67, como 67 $>$ 49, 67 $>$ 52 y 67 $>$ 56 lo inserto como hijo derecho del 56.
\newline
\newline
\textbf{.} El nodo 54, como 54 $>$ 49, 54 $>$ 52 y 54 $<$ 56 lo inserto como hijo izquierdo del 56.
\newline
\newline
\textbf{.} El nodo 77, como 77 $>$ 49, 77 $>$ 52, 77 $>$ 56 y 77 $>$ 67 lo inserto como hijo derecho del 67.
\newline
\newline
\textbf{.} El nodo 83, como 83 $>$ 49, 83 $>$ 52, 83 $>$ 56, 83 $<$ 67 y 83 $>$ 77 lo inserto como hijo derecho del 77.
\newline
\newline
\textbf{.} El nodo 75, como 75 $>$ 49, 75 $>$ 52, 75 $>$ 56, 75 $<$ 67 y 75 $<$ 77 lo inserto como hijo izquierdo del 77.
\newline
\newline
Por lo tanto, obtenemos el siguiente árbol:

\begin{figure}[h]
	\centering
	\includegraphics[width=0.35\textwidth]{abb-2}
	\label{drivers1}
\end{figure}
\newpage

Por lo tanto, el \textbf{costo de la inserción} depende de la  distancia del nodo a la raiz (podriamos decir tambien de la altura del árbol).
\newline
\newline
\textbf{.} En el peor caso, por ejemplo, si tenemos que insertar la secuencia $S_{0}$ = \{1,2,3,4,5,6,7\} el costo es $O(n)$ porque todos los nodos se van a insertando en el subarbol derecho, queda formada una única rama (\textit{árbol desbalanceado}) y la altura del árbol es $n$.
\newline
\newline
\textbf{.} En el caso promedio, por ejemplo, si tenemos que insertar la secuencia $S_{1}$ = \{4,2,3,6,1,5,7\} el costo es $O(lg(n))$ porque todos los nodos se van a insertando de forma tal que se forma un \textit{árbol balanceado}) y la altura esperada del árbol es $lg(n)$.
\newline
\newline
Entonces, si un árbol tiene $n$ nodos, el primer nivel tiene 1 nodo, el segundo nivel tiene 2 nodos, el tercer nivel tiene 4 nodos y por lo tanto, si el árbol tiene altura $h$, el último nivel tiene $2^{h}$ nodos. Por lo tanto:
\newline
\newline
$n$ = $\displaystyle \sum_{i=0}^{h} 2^{i}$
\newline
\newline
\newline
$n$ = $\displaystyle \frac{1-2^{h+1}}{1-2}$ por Serie Geometrica
\newline
\newline
\newline
$n$ = $\displaystyle \frac{1-2^{h+1}}{-1}$
\newline
\newline
\newline
$n$ = $-1+2^{h+1}$
\newline
\newline
$n$ = $2^{h+1} - 1$
\newline
\newline
Y escribiendolo en función de $h$, obtenemos:
\newline
\newline
$n$ = $2^{h+1} - 1$
\newline
\newline
$n + 1$ = $2^{h+1}$
\newline
\newline
$log_{2}(n+1)$ = $log_{2}(2^{h+1})$
\newline
\newline
$log_{2}(n+1)$ = $(h+1)$ . $log_{2}(2)$
\newline
\newline
$log_{2}(n+1)$ = $h+1$
\newline
\newline
$log_{2}(n+1)$ $-$ $1$ = $h$
\newline
\newline
Por lo tanto, podemos acotar $log_{2}(n+1)$ por $lg(n)$. 
\newpage 

\subsubsection{Borrado en ABB}

Queremos implementar la operación \TipoVariable{Borrar(u,A)} donde \TipoVariable{u} es el elemento a borrar y \TipoVariable{A} es el árbol. 
\newline
\newline
Luego, tenemos tres casos para implementar:
\newline
\newline
\textbf{.} $u$ es una hoja
\newline
\newline
\textbf{.} $u$ tiene un solo hijo
\newline
\newline
\textbf{.} $u$ tiene dos hijos
\newline
\newline
Veamos el caso donde $u$ es una hoja. Para ello, debemos:
\newline
\newline
\textbf{.} Buscar al padre
\newline
\newline
\textbf{.} Eliminar la hoja

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-3}
	\label{drivers1}
\end{figure}

Por lo tanto, la complejidad en este tipo de borrado es el equivalente a la búsqueda del padre porque eliminación se realiza en tiempo constante, es decir, la complejidad cuando se borra una hoja es $O(n)$.
\newline
\newline
Ahora, veamos el caso donde se desea borrar un nodo $u$ con un solo hijo $v$. Para ello, debemos:
\newline
\newline
\textbf{.} Buscar al padre $w$ de $u$
\newline
\newline
\textbf{.} Si existe $w$ (si no existe, $u$ es la raíz del árbol y simplemente asignamos como la nueva raíz a $v$), reemplazar la conexión $(w,u)$ con la conexión $(w,v)$.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-4}
	\label{drivers1}
\end{figure}
\newpage

Por último, veamos un ejemplo de aplicación para este caso:

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-5}
	\label{drivers1}
\end{figure}

Ahora, veamos el caso donde se desea borrar un nodo $u$ con dos hijos. Para ello, debemos:
\newline
\newline
\textbf{.} Encontrar el \textit{predecesor inmediato} $v$ o \textit{sucesor inmediato} de $u$
\newline
\newline
$v$ no puede tener dos hijos, en caso contrario no sería el predecesor inmediato (sucesor).
\newline
\newline
\textbf{.} Copiar la clave de $v$ en lugar de la $u$
\newline
\newline
\textbf{.} Borrar el nodo $v$. $v$ es hoja, o bien tiene un solo hijo, lo que nos lleva a los casos anteriores.
\newline
\newline
Veamos el siguiente ejemplo considerando el predecesor inmediato:

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-6}
	\label{drivers1}
\end{figure}

Por último, el costo del borrado en un ABB requiere de dos operaciones:
\newline
\newline
\textbf{.} El borrado de un nodo interno requiere encontrar al nodo que hay que borrar ($O(n)$) y a su predecesor inmediato ($O(n)$).
\newline
\newline
En el peor caso, ambos costos son lineales: $O(n)$ $+$ $O(n)$ = $O(n)$
\newpage

Anteriormente, vimos que la complejidad en la inserción de un ABB depende de la altura $h$ del mismo por lo que si pudiesemos controlar la profundidad del árbol, estariamos pudiendo controlar la complejidad de las operaciones. Por lo tanto, vamos a querer que el árbol siempre se encuentre \textbf{balanceado}. 

\subsection{Introducción al balanceo}

¿Qué altura tiene un árbol completo? Un árbol de $n$ nodos tiene una altura de $log_{2}(n)$.
\newline
\newline
Pero no podemos pretender tener siempre árboles completos porque resulta demasiado caro mantenerlos debido a las posibles inserciones y borrados.
\newline
\newline
Por lo tanto, querriamos implementar árboles $k$-arios donde:
\newline
\newline
\textbf{.} Cada nodo tiene 0 o $k$ hijos
\newline
\newline
\textbf{.} La longitud de dos ramas cualesquiera difieran en a lo sumo en una unidad

\subsection{Balanceo perfecto}

\textbf{Teorema}: Un arbol binario \textit{perfectamente balanceado} de $n$ nodos tiene altura $lg_{2}(n) + 1$
\newline
\newline
\textbf{Demostración (por inducción)}
\newline
\newline
Vamos a utilizar el siguiente lema:
\newline
\newline
Si cada nodo tiene 0 o 2 hijos $n_{h}$ = $n_{i} + 1$ donde:
\newline
\newline
$n_{h}$ = \#hojas, $n_{i}$ = \#nodos internos y $n$ = $n_{h}$ + $n_{i}$  
\newline
\newline
Entonces, comenzamos demostrando que $n_{h}$ = $n_{i} + 1$ 
\newline
\newline
Como cada nodo tiene 0 o 2 hijos, si a una hoja le agregamos 2 hijos nuevos, tenemos 2 hijos más, 1 hoja que desaparece porque ahora se suma a los nodos internos. Por lo tanto, inductivamente luego de realizar esta operación, obtenemos que:
\newline
\newline
$n_{h}'$ = $n_{h}$ + 2 (hojas nuevas) - 1 (pasó a ser nodo interno) 
\newline
\newline
$n_{h}'$ = $n_{h}$ + 1 y por H.I $n_{h}$ = $n_{i} + 1$ y por lo tanto:
\newline
\newline
$n_{h}'$ = ($n_{i} + 1$) + 1 y por lo tanto ($n_{i} + 1$) es el nuevo número de nodos internos. Entonces:
\newline
\newline
$n_{h}'$ = $n_{i}'$ + 1
\newline
\newline
Por lo tanto, probamos que el número de hojas es igual al numero de nodos internos + 1.
\newline
\newline
Luego, como el número de hojas es igual al número de nodos internos + 1 y $n$ = $n_{h}$ + $n_{i}$, podemos concluir que las hojas son más del 50\% de los nodos del árbol.
\newline
\newline
Con el lema probado, vamos a demostrar el teorema. Para ello, podamos el árbol eliminando primero las hojas de las ramas más largas. Luego, podamos todas las hojas y nos queda otro árbol con las mismas características (todas las ramas tienen la misma longitud y todos los nodos tiene 0 o $k$ (2 en nuestro caso) hijos) y como cada vez que podemos una hoja, vamos a eliminar mas de la mitad de los nodos por el lema que recién probamos, vamos a poder realizar $lg_{2}(n)$ podas y por lo tanto, probamos que la altura de estos arboles es $lg_{2}(n)$ + 1.  
\newline
\newline
Por lo tanto, el costo de búsqueda/inserción/borrado es $O(lg(n))$.
\newline
\newline
Pero sucesiones de inserciones y borrados pueden destruir el balanceo.
\newpage

\subsection{Balanceo en altura. Árboles AVL}

Un árbol se dice \textbf{balanceado en altura} si las alturas de los subárboles izquierdo y derecho de \textbf{cada nodo} difieren en a lo sumo una unidad.
\newline
\newline
A diferencia del \textit{balanceo perfecto} donde las diferencias de altura de todas las ramas fueran de a lo sumo una unidad, en este caso, estamos pidiendo que la rama más larga (altura) del subárbol derecho difiera de la rama más larga (altura) del subárbol izquierdo en a lo sumo una unidad. 
\newline
\newline
Los árboles balanceados en altura se llaman \textbf{árboles AVL}.
\newline
\newline
Por lo tanto, consideramos un árbol AVL si:
\newline
\newline
\textbf{.} Es un árbol ABB
\newline
\newline
\textbf{.} $|$FDB$|$ $\leq$ 1 para cada nodo
\newline
\newline
Ahora, queremos ver que \textbf{tan altos son los AVL}.
\newline
\newline
Si queremos demostrar que la altura de un árbol con $n$ nodos es menor que $f(n)$, eso es lo mismo que demostrar que un árbol con altura $x$ tiene más que $f^{-1}(x)$ nodos.
\newline
\newline
Por lo tanto, si queremos demostrar que la altura de un árbol con $n$ nodos es menor que $log(n)$, eso es lo mismo que demostrar que un árbol con altura $x$ tiene más que $exp(x)$ nodos. 
\newline
\newline
Los árboles AVL con el mínimo número de nodos dada la altura se llaman \textbf{árboles de Fibonacci}. Por lo tanto:
\newline
\newline
\textbf{.} El mínimo número de nodos de un árbol de altura 1 es 1 (solo la raíz).
\newline
\newline
\textbf{.} El mínimo número de nodos de un árbol de altura 2 es 2 (la raíz más un hijo derecho o izquierdo).
\newline
\newline
\textbf{.} El mínimo número de nodos de un árbol de altura 3 es 4 (la raíz más un subárbol izquierdo de altura 2 y un subárbol derecho de altura 1).
\newline
\newline
\textbf{.} El mínimo número de nodos de un árbol de altura 4 es 7 (la raíz más un subárbol izquierdo correspondiente al árbol de Fibonacci de altura 3 y un subárbol derecho correspondiente al árbol de Fibonacci de altura 2).
\newline
\newline
La cantidad de nodos de un árbol AVL con altura $i+2$ con mín cantidad de nodos es $AVL_{i+2}$ = $AVL_{i+1}$ + $AVL_{i}$ + 1   
\newline
\newline
Por lo tanto, podemos construir la siguiente tabla con la cantidad mínima de nodos de un árbol AVL de altura $h$:

\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{abb-7}
	\label{drivers1}
\end{figure}
\newpage
Donde se obtienen las siguientes relaciones:
\newline
\newline
$AVL_{i+2}$ = $AVL_{i+1}$ + $AVL_{i}$ + 1
\newline
\newline
$F_{i+2}$ = $F_{i}$ + $F_{i+1}$ 
\newline
\newline
$AVL_{i}$ = $F_{i+2}$ - 1
\newline
\newline
Luego, la función de Fibonacci es exponencial en $i$, la altura de un árbol AVL con la mínima cantidad de nodos es exponencial dada la altura, la máxima altura es logaritmica.   
\newline
\newline
Un árbol de Fibonacci tiene todos los factores de balanceo de sus nodos internos $\pm$ 1. Es el árbol balanceado más cercano a la condición de no balanceo. 
\newline
\newline
Un árbol de Fibonacci con $n$ nodos tiene altura $<$ 1.44 $log(n+2)$ - 0.328. Por lo tanto, un árbol AVL de $n$ nodos tiene altura $\Theta(lg(n))$.

\subsubsection{Inserción en Árboles AVL}

Antes de realizar las inserciones, veamos el concepto de las \textbf{rotaciones} con la siguiente imagen: 

\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{abb-8}
	\label{drivers1}
\end{figure}

\textbf{.} En el arbol de la izquieda se aplica una \textbf{rotación a izquierda (RL(x))} en el nodo $x$ generando como resultado el subarbol de la derecha.
\newline
\newline
\textbf{.} En el arbol de la derecha se aplica una \textbf{rotación a derecha (RR(y))} en el nodo $y$ generando como resultado el subarbol de la izquierda.
\newline
\newline
Entonces, veamos el siguiente ejemplo:

\begin{figure}[h]
	\centering
\includegraphics[width=0.25\textwidth]{abb-9}
	\label{drivers1}
\end{figure}
\newpage

Insertamos el nodo número 22 y observamos que se rompe el factor de balanaceo. Para ello, aplicamos una rotación a derecha (RR) sobre el nodo 28 donde $y$ = 28 y $x$ = 25 y $A$ = 22 obteniendo el siguiente resultado:

\begin{figure}[h]
	\centering
\includegraphics[width=0.25\textwidth]{abb-10}
	\label{drivers1}
\end{figure}

Y podemos observar que nuevamente el arbol cumple con el factor de balanceo.
\newline
\newline
Ahora, queremos insertar el nodo número 55 y lo realizamos de la siguiente manera:

\begin{figure}[h]
	\centering
\includegraphics[width=0.25\textwidth]{abb-11}
	\label{drivers1}
\end{figure}

Sin embargo, podemos observar que el nodo 67 rompre el factor de balanceo (-2).
\newpage
Ahora, para solucionar esto aplicamos primero una rotación a izquierda sobre el nodo 51 y luego una rotación a derecha sobre el nodo 67 logrando mantener nuevamente el factor de balanceo.  

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-12}
	\label{drivers1}
\end{figure}

Por lo tanto, para insertar un nodo en un árbol AVL, debemos:
\newline
\newline
1. Insertar el nuevo nodo como en un ABB clasico. El nuevo nodo es una hoja.
\newline
\newline
2. Recalcular los factores de balanceo que cambiaron por la inserción. Solo en la rama en la que ocurrió la inserción, de abajo hacia arriba.
\newline
\newline
3. Si en la rama aparece un factor de balanceo $\pm$ 2 hay que rebalancear a través de las \textbf{rotaciones}.
\newline
\newline
\textbf{Rotaciones en los árboles AVL}
\newline
\newline
Los casos posibles de rotaciones son:
\newline
\newline
\textbf{.} RR: inseerción en el subarbol derecho de un hijo derecho (del nodo que se desbalancea).
\newline
\newline
\textbf{.} LR: inseerción en el subarbol izquierdo de un hijo derecho (del nodo que se desbalancea).
\newline
\newline
\textbf{.} RL: inseerción en el subarbol derecho de un hijo izquierdo (del nodo que se desbalancea).
\newline
\newline
\textbf{.} LL: inseerción en el subarbol izquierdo de un hijo izquierdo (del nodo que se desbalancea).
\newline
\newline
\textbf{Costo de la inserción en los árboles AVL}
\newline
\newline
\textbf{.} Paso 1: Proporcional a la altura del árbol $\Theta(log(n))$
\newline
\newline
\textbf{.} Paso 2: Proporcional a la altura del árbol $\Theta(log(n))$
\newline
\newline
\textbf{.} Paso 3: $O(1)$ se hace una o dos rotaciones por inserción
\newline
\newline
En total, $\Theta(log(n))$. 
\newpage

\subsubsection{Borrado en Árboles AVL}

Para borrar un nodo en un árbol AVL, debemos:
\newline
\newline
1. Borrar el nuevo nodo como en un ABB clasico. \newline
\newline
2. Recalcular los factores de balanceo que cambiaron por la inserción. Solo en la rama en la que ocurrió el borrado, de abajo hacia arriba.
\newline
\newline
3. Para cada nodo con factor de balanceo $\pm$ 2 hay que rebalancear a través de las \textbf{rotaciones} simples o dobles donde tenemos $O(lg(n))$ rotaciones en el peor caso.
\newline
\newline
\textbf{Ejemplo}
\newline
\newline
Supongamos que tenemos el siguiente árbol AVL que se encuentra balanceado:

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-13}
	\label{drivers1}
\end{figure}

Ahora, queremos borrar el nodo número 7.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-14}
	\label{drivers1}
\end{figure}
\newpage
Como el nodo número 7 es una hoja lo borramos simplemente y calculando nuevamente los factores de balanceo, obtenemos que el arbol se encuentra balanceado. 

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-15}
	\label{drivers1}
\end{figure}

Ahora, queremos eliminar el nodo número 6.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-16}
	\label{drivers1}
\end{figure}
\newpage

Como el nodo 16 tiene un solo hijo, buscamos su padre y lo conectamos con su hijo (nodo número 2) obteniendo el siguiente árbol que recalculando los factores de balanceo, se encuentra balanceado. 

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-17}
	\label{drivers1}
\end{figure}

Ahora, queremos borrar el nodo número 14.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-18}
	\label{drivers1}
\end{figure}
\newpage
Estamos en el caso de un nodo con dos hijos. Por lo tanto, buscamos el predecesor inmediato y lo reemplazamos en la posición del nodo a eliminar obteniendo el siguiente árbol:

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-19}
	\label{drivers1}
\end{figure}

En este caso, el árbol rompe el factor de balanceo en el nodo número 2 (+2). Tenemos un balanceo de tipo RR y lo solucionamos con una rotación a izquierda aplicada en el nodo número 2 obteniendo el siguiente árbol que cumple con el factor de balanceo:

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-20}
	\label{drivers1}
\end{figure}
\newpage
Ahora, queremos eliminar el nodo número 18.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-21}
	\label{drivers1}
\end{figure}

Como el nodo 18 es una hoja, lo borramos simplemente y verificamos que se cumple el factor de balanceo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-22}
	\label{drivers1}
\end{figure}
\newpage
Ahora, queremos eliminar el nodo número 75. Como es una hoja lo eliminamos simplemente y y verificamos que se cumple el factor de balanceo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-23}
	\label{drivers1}
\end{figure} 

Ahora, queremos eliminar el nodo número 19. Como es una hoja lo eliminamos simplemente pero no se cumple el factor de balanceo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-24}
	\label{drivers1}
\end{figure} 
\newpage
Estamos en un caso de balanceo LR. Lo solucionamos con una rotación doble. Primero, aplicando una rotación a derecha sobre el nodo 50 y luego una rotación a izquierda sobre el nodo 21 obteniendo el siguiente árbol completo que ahora verifica el factor de balanceo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abb-25}
	\label{drivers1}
\end{figure} 

\textbf{Costo del borrado en los árboles AVL}
\newline
\newline
En el caso peor hay que hacer rotaciones (simples o dobles) a lo largo de toda la rama.
\newline
\newline
\textbf{.} Paso 1: Borrado como en un ABB. Proporcional a la altura del árbol $\Theta(log(n))$
\newline
\newline
\textbf{.} Paso 2: Recalculo de los factores de balanceo. Proporcional a la altura del árbol $\Theta(log(n))$
\newline
\newline
\textbf{.} Paso 3: Rotaciones simples o dobles. $\Theta(log(n))$ . $\Theta(1)$
\newline
\newline
En total, $\Theta(1)$. 

\section{Clase Teórica 06. Tries}

La motivación para esta clase es:
\newline
\newline
\textbf{.} Buscar una implementación de diccionarios menos dependiente de la cantidad de claves.
\newline
\newline
\textbf{.} La idea es independizarnos de la cantidad de claves pero manteniendo un rendimiento razonable en el peor caso enfocandonos en el tamaño de las claves y un poco menos en el tamaño del conjunto.
\newline
\newline
\textbf{.} Lograr estructuras de datos muy rápidas en la práctica.
\newline
\newline
\textbf{.} Adecuadas para claves de tamaño variable.

\subsection{Idea}

La idea es no hacer comparaciones de claves completas, sino de \textbf{partes} de ellas. Por ejemplo, si las claves son números enteros, las \textit{partes} podrían ser los dígitos/bits y si las claves son strings, las \textit{partes} podrían ser los caracteres.

\subsection{Desventajas}

Las principales desventajas de estas implementaciones son:
\newline
\newline
Algunas implementaciones pueden requerir mucha memoria.
\newline
\newline
Las operaciones sobre las componentes de las claves puede no ser fácil, o ser muy ineficiente en algunos lenguajes de alto nivel.

\subsection{Árboles de búsqueda dígital}

La primera estructura con acercamiento a los \textit{tries} son los árboles de búsqueda dígital. 
\newline
\newline
Son parecidos a los ABB. Para guardar una clave, vamos recorriendo el camino en el árbol que nos lleva a su posición.
\newline
\newline
Pero la posición no está determinada por comparaciones $>$ o $<$ sino por los bits, dígitos o caracteres, es decir, por las \textbf{partes} de la clave.
\newpage
Veamos el siguiente ejemplo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{abd-1}
	\label{drivers1}
\end{figure} 

Como el árbol está inicialmente vacio, insertamos la clave E como raíz.
\newline
\newline
Luego, como la clave J comienza con el bit 0, lo colocamos como hijo izquierdo de la raíz.
\newline
\newline
Como la clave M comienza con el bit 0, lo colocamos como hijo izquierdo de la raíz. Pero como ya se encuentra ocupado por la letra J, el siguiente bit es 1 y lo insertamos como hijo derecho del nodo J.
\newline
\newline
Como la clave P comienza con el bit 1, lo colocamos como hijo derecho de la raíz. 
\newline
\newline
Como la clave L comienza con el bit 0, lo colocamos como hijo izquierdo de la raíz. Pero como ya se encuentra ocupado por la letra J, el siguiente bit es 1 y lo colocamos como hijo derecho del nodo J. Pero como tambien ya se encuentra ocupado por la letra M, el siguiente bit es 1 y lo colocamos como hijo derecho del nodo M.
\newline
\newline
Como la clave O comienza con el bit 0, lo colocamos como hijo izquierdo de la raíz. Pero como ya se encuentra ocupado por la letra J, el siguiente bit es 1 y lo colocamos como hijo derecho del nodo J. Pero como ya se encuentra ocupado por la letra M, el siguiente bit es 1 y lo colocamos como hijo derecho del nodo M. Como tambien ya se encuentra ocupado por la letra L, el siguiente bit es 1 y lo colocamos como hijo derecho del nodo L.
\newline
\newline
Y siguiendo este patrón, insertamos las claves restantes obteniendo el árbol que se muestra en la imagen.

\subsubsection{Propiedades}

El peor caso es mucho mejor que el de los ABBs. El número de claves es grande, es decir, que si todas las claves viven en un conjunto $U$, entonces $n$ $<$ $|$ $U$ $|$, es decir, que el número de nodos es menor al cardinal del conjunto $U$ que contiene todas la claves. Por otro lado, si las claves no son largas, la altura del árbol va a ser pequeña y va a estar dada por la cantidad de bits que conforman a la claves, es decir, si las claves se encuentran el conjunto $U$, el tamaño de cada clave está dado por $log(|U|)$. 
\newline
\newline
Tambien podemos afirmar que la longitud del camino más largo es igual al mayor número de bits sucesivos iguales de dos claves cualesquiera a partir del bit más a la izquierda (o sea, mayor prefijo común).
\newline
\newline
La búsqueda o inserción en un árbol de $n$ claves de $b$ bits, necesita en promedio (suponiendo la distribución equitativa de las claves) $log(n)$ comparaciones de clave completa porque cada vez nos paramos en un nodo, tenemos que mirar la clave completa para ver si ese era el nodo que estabamos buscando, y $b$ en el peor caso.
\newpage

\subsection{Tries}

Con base en los Árboles de Búsqueda Dígital, vamos a introducir una nueva estructura llamada \textbf{tries}.
\newline
\newline
Un \textbf{trie} es un árbol $k+1$-ario para alfabetos de $k$ elementos.
\newline
\newline
Ahora, la información no va a estar asociada a los nodos sino a los \textbf{ejes} del árbol dado que representan componentes de las claves.
\newline
\newline
Por ejemplo:
\newline
\newline
\textbf{.} Si las claves son $strings$, los ejes representan caracteres.
\newline
\newline
\textbf{.} Si las claves son $enteros$, los ejes representan dígitos o bits.
\newline
\newline
En un trie, cada subárbol representa al conjunto de claves que comienza con las etiquetas de los ejes llevan hasta él. Podriamos considerar esta definición como la función de abstracción.
\newline
\newline
Los nodos internos no contienen claves.
\newline
\newline
Las claves o los punteros a la información se almacenan en las hojas.
\newline
\newline
Veamos el siguiente ejemplo de \textit{trie}.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{trie-1}
	\label{drivers1}
\end{figure} 

Tenemos un diccionario que contiene cinco palabras (BIG, BIGGER, BILL, GOOD, GOSH).
\newline
\newline
Inicialmente, en el subárbol derecho se encuentra la secuencia vacia.
\newline
\newline
En el subárbol izquierdo se encuentran todas las palabras que comienzan con B. El primer nodo almacena todas las palabras que comienzan con B. luego, si seguimos avanzando, nos encontramos con el nodo que almacena todas las palabras que comienzan con BI. Si seguimos avanzando hacia el hijo izquierdo, nos encontramos con las palabras que comienzan con BIG; y si avanzamos hacia el hijo derecho nos encontramos con las palabras que comienzan con BIL. Ahora, si seguimos avanzando por la letra L, nos encontramos con todas las palabras que comienzan con BILL. Y como BILL es una palabra que queremos insertar se añade otro eje (convierte al árbol en $k+1$-ario) y su hoja contiene la palabra BILL. Podemos observar el mismo caso para BIG donde se crea un eje con el signo \$ indicando que la hoja que contiene es la palabra BIG y en los demás ejes se almacenan las palabras que comienzan con BIG.
\newline
\newline
En el subárbol del centro se encuentran las palabras que comienzan con G. Y el análisis que se realiza es el mismo que para el caso de B.
\newpage

\subsubsection{Propiedades de los tries}

A continuación, veamos algunas propiedades de los \textit{tries}.
\newline
\newline
\textbf{.} Para buscar o insertar, tenemos que hacer, a lo sumo, una comparación de clave completa cuando llego a una hoja
\newline
\newline
\textbf{.} A diferencia de los ABB y los ABD, la estructura del trie es la misma independientemente del orden en que se insertan las claves, o sea, hay un único trie para un conjunto de claves.
\newline
\newline
\textbf{.} La búsqueda/inserción en un trie construido a partir de $n$ claves de $b$ bits, necesita en promedio $log(n)$ comparaciones de bits en promedio (suponiendo la distribución equitativa de las claves) y $b$ comparaciones en el peor caso.

\subsection{Implementación de tries}

Ahora, nos vamos a enfocar en como representar los $tries$.
\newline
\newline
\textbf{.} La primer posibilidad es tener en cada nodo interno, un arreglo de punteros. Caso que utilizamos \textit{memoria estática}. Veamos el siguiente ejemplo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{trie-2}
	\label{drivers1}
\end{figure}

Observamos que cada clave contiene un puntero que apunta a otro arreglo de punteros en el caso de haya palabras que comiencen con esa clave. Caso contrario, el puntero apunta a $nil$. Y en todos los casos, tenemos una posición adicional en el array para el puntero que representa al subárbol de la palabra que terminó allí.
\newline
\newline
Por lo tanto, la implementacion con arreglos es muy eficiente en términos de tiempo, el algoritmo es simple pero es extremadamente ineficiente en términos de memoria porque solo en el caso de las letras, estamos reservando 26 posiciones para cada arreglo cuando sólo podriamos llegar a utilizar una o dos posiciones. Es decir, esto ocurre cuando el alfabeto es grande y el diccionario es chico. Para solucionar esto, se podría agrupar caracteres pocos frecuentes. 
\newpage
\textbf{.} La segunda posibilidad es tener para cada nodo, un puntero a sus hijos y además, cada nodo puede estar en una lista con sus hermanos. Caso que utilizamos \textit{memoria dinámica}. Veamos el siguiente ejemplo.

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{trie-3}
	\label{drivers1}
\end{figure}

Podemos observar que, cada nodo, tiene dos punteros. Uno al mayor de sus hijos (puntero rojo) y otro al que le sigue en su lista de hermanos (puntero azul).
\newline
\newline
Por lo tanto, la implementacion con punteros es eficiente en términos de tiempo solo si hay pocas claves porque en lugar de hacer un solo acceso a claves como en lugar, tenemos que recorrer potencialmente toda la lista de manera secuencial. Por ejemplo, si queremos ver si existen claves que comiencen con la letra Z, tendriamos que recorrer desde el nodo que contiene a B, pasar por el de G y asi hasta llegar a Z. Debido a esto, requiere algoritmos sobre manejo de listas pero es mucho mas eficiente en términos de memoria. 

\subsection{Tries compactos}

Son parecidos a los tries tradicionales pero colapsamos las cadenas que llevan hacia hojas.

\begin{figure}[h]
	\centering
\includegraphics[width=0.63\textwidth]{trie-4}
	\label{drivers1}
\end{figure}
\newpage

\subsection{Tries más compactos: Patricia}

Ahora colapsamos todas las cadenas. Por ejemplo, vemos que todas las palabras que comienzan con B, continuan con BI y todas las palabras que comienzan con G, continuan con GO. 
\newline
\newline
Además, un eje puede representar más de un caracter.

\begin{figure}[h]
	\centering
\includegraphics[width=0.63\textwidth]{trie-5}
	\label{drivers1}
\end{figure}

La altura de un árbol Patricia binario está acotada por $n$ (el número de claves). Eso no sucede con los modelos anteriores. 
\newpage

\section{Clase Teórica 07. Diseño de conjuntos y diccionarios con Hashing}

\subsection{Tablas Hash}

Son adecuadas para representar diccionarios.
\newline
\newline
Representan la generalización del concepto de arreglo.
\newline
\newline
Importantes para el acceso a datos en memoria secundaria.
\newline
\newline
\textbf{.} Los accesos se dan en memoria secundaria
\newline
\newline
\textbf{.} El costo de los accesos es el predominante

\subsection{Direccionamiento directo}

Supongamos el caso donde tenemos que almacenar perfiles de clientes y tengo que encontrar alguna manera de indexarlos en un diccionario por su clave. 
\newline
\newline
Una opción posible de clave es considerar el número de DNI dado que es único por persona. Para ello, puedo considerar tener un arreglo de longitud $10^{8}$ para almacenar los DNI's que van desde el 0 hasta el 99.999.999 
\newline
\newline
Cuando quiero consultar, por ejemplo, por el DNI 28.543.897, basta con acceder a esa posición del arreglo y obtener el DNI.
\newline
\newline
La ventaja de esto es que puedo acceder a cualquier DNI en tiempo $O(1)$. Pero, el problema es que se produce mucho desperdicio de memoria porque si solo necesito registrar 10000 clientes, estoy utilizando 100000000 de entradas para solo 10000 clientes. 

\begin{figure}[h]
	\centering
\includegraphics[width=0.5\textwidth]{hash-1}
	\label{drivers1}
\end{figure}

Sin embargo, podemos ver estamos indexando números de DNI pero estamos buscando la generalización de los arreglos y no indexar unicamente números enteros positivos.
\newpage

\subsection{Objetivos}

Por lo tanto, queremos:
\newline
\newline
\textbf{.} Indexar otros tipos de datos (no sólo enteros)
\newline
\newline
\textbf{.} $n$ = \# claves efectivamente usadas 
\newline
\newline
\textbf{.} Tiempo de búsqueda: $O(1)$
\newline
\newline
Observemos que la \# claves posibles puede ser mucho mayor que la \# claves efectivamente usadas

\subsubsection{Indexar otros tipos de datos}

Necesitamos de una función de correspondencia entre cualquier tipo de datos y un entero. A esto, se lo denomina \textbf{Pre-hashing}. 

\subsubsection{$n$ = \# claves efectivamente usadas}

Vamos a representar un diccionario como una tupla $\langle T,h \rangle$ donde: 
\newline
\newline
\textbf{.} $T$ es un arreglo con $N$ = tam($T$) celdas.
\newline
\newline
\textbf{.} La función $h$ (hash) que mapea del universo de claves $K$ al arreglo especifico de tamaño $N$, es decir, $h$:$K$ $\rightarrow$ \{0,...,$N-1$\}

\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{hash-2}
	\label{drivers1}
\end{figure}

Tenemos el universo de claves $K$, las claves que estoy manipulando en un momento dado ($k_{0}$,$k_{1}$,$k_{2}$,$k_{3}$) y la función de hash que, al aplicarla sobre cada una de esas claves, me ubica en una posición correspondiente en el arreglo como vemos a continuación. 

\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{hash-3}
	\label{drivers1}
\end{figure}

Podemos observar que el gran problema es encontrar la función $h$ donde el objetivo principal es que la función $h$ indexe a a cada clave en una posición que se encuentre libre en el la tabla $T$.
\newpage

Entonces:
\newline
\newline
La función de \textbf{hash perfecta} propone que si tengo dos claves distintas $k_{1}$,$k_{2}$, al aplicarle la función de hash, $h(k_{1})$ y $h(k_{2})$ tambien son distintas. Es decir, $k_{1}$ $\neq$ $k_{2}$, entonces $h(k_{1})$ $\neq$ $h(k_{2})$.
\newline
\newline
Requiere que $N$ $\geq$ $|$$K$$|$ y es raramente razonable en la práctica porque generalmente $|$$K$$|$ $>$$>$ $N$.
\newline
\newline
Por lo tanto, ahora nos vamos a encontrar con claves distintas que, al aplicarles la función de hash $h$, se mapeen a la misma posición del arreglo. A este fenomeno lo llamamos \textbf{colisión}. 
\newline
\newline
\textbf{Resolución de colisiones}
\newline
\newline
Los métodos se diferencian por la forma de ubicar a los elementos que dan lugar a colisión. Tenemos dos familias principales:
\newline
\newline
\textbf{.} Direccionamiento cerrado o concatenación: a la $i$-esima posición de la tabla se le asocia la lista de los elementos tales que $h(k)$ = $i$.
\newline
\newline
\textbf{.} Direccionamiento abierto: todos los elementos se guardan en la tabla.
\newline
\newline
\textbf{Requisitos de una función de hash}
\newline
\newline
Entonces para evitar las colisiones, tratamos de encontrar una uniformidad simple, es decir, tratar de que las claves se distribuyan lo más equitativamente posible entre todas las posiciones del arreglo. Es decir:
\newline
\newline
$\forall$ $j$ $\displaystyle \sum_{k \in K, h(k)=j}^{}$$P(k)$ $\approx$ $\displaystyle \frac{1}{|N|}$
\newline
\newline
Para todo $j$ donde $j$ son las posiciones del arreglo, si sumamos sobre todas las claves tales que la función de hash la envio a la posición $j$, la probabilidad de las claves tiene que ser igual o converger a $\displaystyle \frac{1}{|N|}$ donde $N$ es el tamaño de la tabla. Es decir, si agrupamos a todos los que van a parar al mismo lugar tiene que ser igual a $\displaystyle \frac{1}{|N|}$ del total de las claves.    
\newline
\newline
Pero, el problema es que esta uniformidad es dificil de conseguir porque $P$ es generalmente desconocida, es decir, no se conoce la distribución de $P$. 
\newline
\newline
Para ver esto, veamos el siguiente ejemplo.
\newline
\newline
Sea $|$$T$$|$=5 (longitud de la tabla) y $h(k)$ = $k$ mod $5$. Entonces, queremos mapear los siguientes conjuntos: \{1,7,10,14\} y \{1,6,11,16\} 
\newline
\newline
Comencemos analizando el caso de \{1,7,10,14\}.
\newline
\newline
\textbf{.} $h(1)$ = 1 mod 5 = 1
\newline
\newline
\textbf{.} $h(7)$ = 7 mod 5 = 2
\newline
\newline
\textbf{.} $h(10)$ = 10 mod 5 = 0
\newline
\newline
\textbf{.} $h(14)$ = 14 mod 5 = 4
\newline
\newline
En este caso, cada clave se ubica en una posición diferente y obtenemos la uniformidad simple.
\newpage
Ahora, analicemos el caso de \{1,6,11,16\}.
\newline
\newline
\textbf{.} $h(1)$ = 1 mod 5 = 1
\newline
\newline
\textbf{.} $h(6)$ = 6 mod 5 = 1
\newline
\newline
\textbf{.} $h(11)$ = 11 mod 5 = 1
\newline
\newline
\textbf{.} $h(16)$ = 16 mod 5 = 1
\newline
\newline
En este caso, todas las claves se ubican en la posición 1, es decir, se produce aglomeración de los elementos.
\newline
\newline
\textbf{Método de hashing por concatenación}
\newline
\newline
Comencemos viendo el siguiente ejemplo. Supongamos que tenemos 5 claves $k_{1}$ = 0, $k_{2}$ = 4, $k_{4}$ = 10, $k_{5}$ = 9, $k_{7}$ = 14 y tenemos la función de hash $h(k)$ = $k$ mod 5 y queremos insertarlas en una tabla $T$ donde $|$$T$$|$ = 5.
\newline
\newline
Por lo tanto:
\newline
\newline
\textbf{.} $h(k_{1})$ = 0 mod 5 = 0
\newline
\newline
\textbf{.} $h(k_{2})$ = 4 mod 5 = 4
\newline
\newline
\textbf{.} $h(k_{4})$ = 10 mod 5 = 0
\newline
\newline
\textbf{.} $h(k_{5})$ = 9 mod 5 = 4
\newline
\newline
\textbf{.} $h(k_{7})$ = 14 mod 5 = 4
\newline
\newline
Entonces, $h(k_{1})$ = $h(k_{4})$ = 0 y $h(k_{2})$ = $h(k_{5})$ = $h(k_{7})$ = 4
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{conc-1}
	\label{drivers1}
\end{figure}

Estas \textbf{colisiones} las resolvemos mediante el método de concatenación. Es decir, en cada posición de la tabla, no hay claves sino un puntero a una lista de todas las claves que están en el diccionario y luego de aplicarles la función de hash, fueron enviadas a esa posición.
\newline
\newline
\textbf{Complejidad de los algoritmos para el método de hashing por concatenación}
\newline
\newline
\textbf{.} Insertar($el,k$): Inserción al principio de la lista asociada a la posición $h(k)$: costo $O(1)$.
\newline
\newline
\textbf{.} Buscar($k$): Búsqueda lineal en la lista asociada a la posición $h(k)$. Por lo tanto, el costo es \textit{O(longitud de la lista asociada a h(k))}.
\newline
\newline
\textbf{.} Borrar($k$): Búsqueda en la lista asociada a la posición $h(k)$. Por lo tanto, el costo es \textit{O(longitud de la lista asociada a h(k))}.
\newline
\newline
¿Cuánto miden las listas?
\newline
\newline
El peor caso ocurren cuando todas las claves una vez aplicada la función de hash, son enviadas a la posición de la tabla. Por lo tanto, la complejidad es $O(n)$ donde $n$ es la cantidad de claves totales. 
\newline
\newline
En el caso promedio, si tenemos una función de hash que distribuye bien los elementos en la tabla, vamos a tener una mejor situación que en el caso peor. 
\newline
\newline
Por lo tanto, si:
\newline
\newline
\textbf{.} $n$ = \# elementos/claves en la tabla 
\newline
\newline
\textbf{.} $\alpha$ = $\displaystyle \frac{n}{|T|}$ factor de carga
\newline
\newline
\newline
\textbf{Teorema}: Bajo la hipótesis de uniformidad simple de la función de hash, si las colisiones se resuelven por concatenación, en promedio:
\newline
\newline
\textbf{.} Una búsqueda fallida requiere tiempo $\Theta(1+\alpha)$
\newline
\newline
\textbf{.} Una búsqueda exitosa requiere tiempo $\Theta(1+\displaystyle \frac{\alpha}{2})$
\newline
\newline
Por útimo si $n$ $\sim$ $|$$T$$|$, entonces las operaciones tienen costo $O(1)$.
\newline
\newline
\textbf{Método de hashing por direccionamiento abierto}
\newline
\newline
Todos los elementos se almacenan en la tabla.
\newline
\newline
Las colisiones se resuelven dentro de la tabla.
\newline
\newline
\textbf{.} Si la posición calculada está ocupada, hay que buscar una posición libre.
\newline
\newline
\textbf{.} Los distintos métodos con direccionamiento abierto se distinguen por el método de barrido que utilizan.
\newline
\newline
\textbf{.} La función hash pasa a depender del número de intentos realizados
\newline
\newline
\textbf{.} Dirección = $h(k,i)$ para el $i$-esimo intento
\newline
\newline
\textbf{.} $h(k,i)$ debe generar todas las posiciones de $T$
\newline
\newline
Comencemos viendo el algoritmo de \textbf{inserción} para este método.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{abierto-1}
	\label{drivers1}
\end{figure}
\newpage
Veamos el siguiente ejemplo de inserción.
\newline
\newline
Supongamos que queremos insertar las claves 7,10,2,12 y tenemos la tabla $T$ con 5 lugares.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{abierto-2}
	\label{drivers1}
\end{figure}

Entonces, comenzamos insertando la clave número 7.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{abierto-3}
	\label{drivers1}
\end{figure}

Como la posición numero 2 se encuentra vacia, lo insertamos sin problemas.
\newpage
Ahora, continuamos insertando la clave número 10.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{abierto-4}
	\label{drivers1}
\end{figure}

Como la posición numero 0 se encuentra vacia, lo insertamos sin problemas.
\newline
\newline
Continuamos insertando la clave número 2. Como en el intento 0, la función de hash me envia la clave a la posición 2 y se encuentra ocupada por el 7, incremento en una unidad el intento y ahora (2+1) mod 5 = 3 y como la posición 3 se encuentra libre, inserto la clave número 2 en la posición 3 de la tabla sin problemas.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{abierto-5}
	\label{drivers1}
\end{figure}
\medskip
\medskip
Por último, insertamos la clave número 12. Como en el intento 0, la función de hash me envia la clave a la posición 2 y se encuentra ocupada por el 7, incremento en una unidad el intento y ahora (12+1) mod 5 = 3 y como la posición 3 se encuentra ocupada por el 2, incremento en una unidad el intento y ahora (12+2) mod 5 = 4 y como la posición 4 se encuentra libre, inserto la clave número 12 en la posición 4 de la tabla sin problemas.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{abierto-6}
	\label{drivers1}
\end{figure}
\newpage
Ahora, continuemos viendo el algoritmo de \textbf{búsqueda} para este método.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.4\textwidth]{busq-1}
	\label{drivers1}
\end{figure}
\medskip
\medskip
Para comprender mejor el algoritmo, veamos el siguiente ejemplo.
\newline
\newline 
Supongamos que queremos buscar las claves número 12 y 30 en la siguiente tabla:
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.2\textwidth]{busq-2}
	\label{drivers1}
\end{figure}
\medskip
\medskip
Para ello, aplicamos la función de hash en el intento 0 pero se encuentra la clave número 7 en la posición 2. Por lo tanto, incremento el intento, la función de hash me devuelve la posición 3 pero en esta posición se encuentra la clave número 2. Entonces, incremento el intento, la función de hash devuelve la posición 4 la cual contiene a la clave número 12.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{busq-3}
	\label{drivers1}
\end{figure}
\newpage
Ahora, continuamos buscando la clave número 30. Para ello, aplicamos la función de hash en el intento 0 pero se encuentra la clave número 10 en la posición 0. Por lo tanto, incremento el intento, la función de hash me devuelve la posición 1 y esta contiene NULL, es decir, esta vacia la celda. De acuerdo al algoritmo, NULL nos indica que debemos terminar la ejecución y finalizar con la búsqueda.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{busq-4}
	\label{drivers1}
\end{figure}
\newpage
Ahora, supongamos la siguiente situación:
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{busq-5}
	\label{drivers1}
\end{figure}

Mi objetivo es buscar la clave número 12 pero si justo se elimina la clave número 2, la posición 3 queda en NULL. Entonces, al iniciar el algoritmo de búsqueda de la clave 12, el primer intento, me envia a la posición 2 y como 7 != 12, incremento en uno el intento. Luego, el intento 1 me envia a la posición 3 pero ahora se encuentra NULL y por lo tanto, debo terminar la búsqueda y retorno que la clave número 12 no se encuentra en la tabla cuando en realidad si lo está. 
\newline
\newline
Por lo tanto, en el algoritmo debemos tener cuidado con \TipoVariable{T[h(k,i)] != null}. Es decir, podemos marcar los elementos como \textit{borrados} en lugar de \textit{null} pero seguimos teniendo algunos problemas.
\newline
\newline
Por lo tanto, vamos a diferenciar los métodos de \textbf{direccionamiento abierto} de acuerdo a la función de \textbf{barrido} que elijan.
\newline
\newline
Nos referimos al \textbf{barrido} cuando una función $h(k,i)$ debe recorrer todas las posiciones de la tabla ante una colisión.
\newline
\newline
Podemos diferenciar tres grandes formas en la que va realizando el barrido:
\newline
\newline
\textbf{.} Barrido linear
\newline
\newline
\textbf{.} Barrido cuadrático
\newline
\newline
\textbf{.} Hashing doble
\newline
\newline
Se diferencian entre si por su complejidad de cálculo y por el comportamiento respecto a los fenomenos de aglomeración.
\newline
\newline
\textbf{Barrido linear}
\newline
\newline
Calculamos $h(k,i)$ = ($h'(k) + i$) mod $|T|$ donde $h'(k)$ es una función de hashing.
\newline
\newline
Se recorren todas las posiciones en la secuencia $T[h'(k)]$, $T[h'(k)+1]$,..., $T[|T|]$, 0, 1, ..., $T[h'(k)-1]$
\newline
\newline
Posibilidad de \textbf{aglomeración primaria}: dos secuencias de barrido que tienen una colisión, siguen colisionando.
\newline
\newline
Los elementos se aglomeran por largos tramos.
\newline
\newline
Veamos ahora, un ejemplo de \textbf{aglomeración primaria}.
\newline
\newline
Tenemos la función de hash $h(k,i)$ = $(h'(k) + i)$ mod 101 con $h'(k)$ = $k$ mod 101.
\newline
\newline
Queremos realizar la siguiente secuencia de inserciones \{2, 103, 104, 105, ...\}
\newline
\newline
Podemos observar que la función $h'(k)$ en el intento 0 envía a la clave número 2 a la posición 2 de la tabla y como se encuentra libre, la inserto sin problemas.
\newline
\newline
Luego, la función $h'(k)$ en el intento 0 envía a la clave número 103 a la posición 2 de la tabla y como se encuentra ocupada por la clave número 2, incremento en uno el intento. Ahora, en el intento 1, $h(103, 1)$ envia la clave a la posición número 3 de la tabla y como se encuentra libre, la inserto sin problemas.  
\newline
\newline
Ahora, la función $h'(k)$ en el intento 0 envía a la clave número 104 a la posición 3 de la tabla y como se encuentra ocupada por la clave número 103, incremento en uno el intento. Ahora, en el intento 1, $h(104, 1)$ envia la clave a la posición número 4 de la tabla y como se encuentra libre, la inserto sin problemas.  
\newline
\newline
Por último, la función $h'(k)$ en el intento 0 envía a la clave número 105 a la posición 4 de la tabla y como se encuentra ocupada por la clave número 104, incremento en uno el intento. Ahora, en el intento 1, $h(105, 1)$ envia la clave a la posición número 5 de la tabla y como se encuentra libre, la inserto sin problemas.  
\newline
\newline
Este fenómeno se denomina \textbf{aglomeración primaria}. Es decir, genero grandes \textit{clusters} porque solo puedo realizar la inserción de manera eficiente en el intento 0 cuando inserto el primer elemento de la secuencia. 
\newline
\newline
\textbf{Barrido cuadrático}
\newline
\newline
$h(k,i)$ = ($h'(k)$ + $c_{1}$$i$ + $c_{2}$$i^{2}$) mod $|T|$ donde $h'(k)$ es una función de hashing y $c_{1}$ y $c_{2}$ son constantes que hay que elegir de manera tal que a recorrer todos los $i$, eventualmente la función va a probar con todos las posiciones de la tabla, es decir, que estas constantes en algún momento, mapeen a todas las posiciones de la tabla.   
\newline
\newline
Veamos algunos ejemplos de funciones de hash para el barrido cuadrático:
\newline
\newline
$h(k,i)$ = $h'(k)$ + $i^{2}$, $h(k, i+1)$ = $h'(k)$ + $(i+1)^{2}$, $i$ = 1,..., $\displaystyle \frac{(|T|-1)}{2}$
\newline
\newline
$h(k,i)$ = $h'(k)$ + $\displaystyle \frac{i}{2}$ + $\displaystyle \frac{i^{2}}{2}$, $|T|$ = $2^{x}$
\newline
\newline
Consideremos que en este caso, los saltos se van produciendo de manera cada vez mas espaciada. Por ejemplo, en el intento 0 considerando la función de hash $h(k,i)$ = ($h'(k)$ + $c_{1}$$i$ + $c_{2}$$i^{2}$) mod $|T|$ suponiendo que $c_{1}$ = $c_{2}$ = 1:
\newline
\newline
\textbf{.} $h(k,0)$ = $h'(k)$ mod $|T|$
\newline
\newline
\textbf{.} $h(k,1)$ = ($h'(k)$ + 2) mod $|T|$
\newline
\newline
\textbf{.} $h(k,2)$ = ($h'(k)$ + 6) mod $|T|$
\newline
\newline
\textbf{.} $h(k,3)$ = ($h'(k)$ + 12) mod $|T|$
\newline
\newline
De esta manera, evitamos los \textit{clusters} de la aglomeración primaria. Pero podemos observar que, al realizar estos saltos, si una inserción cae en colisión, esa secuencia va a seguir colisionando. Es decir, si las claves están hasheadas a una misma posición, esas colisiones se van a ir repitiendo. Este fenomeno se denomina \textbf{aglomeración secundaria}.  
\newline
\newline
\newline
\newline
\textbf{Hashing doble}
\newline
\newline
La idea es que el barrido también dependa de la clave.
\newline
\newline
Proponemos la función de hash $h(k,i)$ = ($h_{1}(k)$ + $i$$h_{2}(k)$) mod $|T|$ donde $h_{1}(k)$ y $h_{2}(k)$ son funciones de hashing.
\newline
\newline
A diferencia de los barridos anteriores, el \textit{hashing doble} no tiene aglomeración primaria y reduce los fenómenos de aglomeración secundaria aunque no los descarta.
\newpage

\subsection{Funciones de hash}

Tenemos muchos metodos para construir funciones de hash. Entre ellos:
\newline
\newline
\textbf{.} División
\newline
\newline
\textbf{.} Partición
\newline
\newline
\textbf{.} Mid-square
\newline
\newline
\textbf{.} Extracción
\newline
\newline
El objetivo es distribuir las claves lo más uniforme posible para minizar las colisiones.

\subsubsection{División}

Consideramos la función $h(k)$ = $k$ mod $|T|$. La complejidad es baja porque solo se realiza una división pero debemos tener cuidado con el valor de $|T|$. Por ejemplo:
\newline
\newline
\textbf{.} No potencias de 2. Si tenemos $|T|$ = $2^{p}$ entonces todas las claves con los $p$ bits menos significativos iguales, colisionan.
\newline
\newline
\textbf{.} No potencias de 10 si las claves son números decimales por el mismo motivo que el caso anterior, las claves podrían ser multiplos de 10.
\newline
\newline
Una buena elección en la práctica podría ser un número primo no demasiado cercano a una potencia de 2. Por ejemplo, $h(k)$ = $k$ mod 701 para $|T|$=2048 valores posibles.

\subsubsection{Partición}

Particionar la clave $k$ en $k_{1}$,$k_{2}$,...,$k_{n}$.
\newline
\newline
Consideramos la función de hash $h(k)$ = $f(k_{1},k_{2},...,k_{n})$
\newline
\newline
Por ejemplo, consideramos que la clave es un número de una tarjeta de crédito y una posible función de hash:
\newline
\newline
4772 6453 7348 $\rightarrow$ \{477, 264, 537, 348\} 
\newline
\newline
$f(477, 264, 537, 348)$ = (477 + 264 + 537 + 348) mod 701
\newline
\newline
$f(477, 264, 537, 348)$ = 224

\subsubsection{Extracción}

Propone usar solamente una parte de la clave para calcular la dirección.
\newline
\newline
Por ejemplo, las 6 cifras centrales del número de tarjeta de crédito. Es decir, 4772 6453 7348 $\rightarrow$ 264537
\newpage
\section{Clase Teórica 08. Colas de Prioridad y Heaps}

\subsection{Colas de prioridad}

La prioridad en general la expresamos con un entero pero puede ser cualquier otro tipo $\alpha$ con un orden $<_{\alpha}$ asociado.

\subsection{Representación de Colas de prioridad}

La implementación más eficiente de las colas de prioridad es a través de \textbf{heaps}.
\newline
\newline
Un \textbf{heap} es un tipo de ordenamiento de mis elementos que no es del todo ordenado como los árboles AVL o una lista ordenada pero lo suficiente como para extraer facilmente el máximo o míinimo y de esa manera, se vuelva a armar el invariante que permite que el próximo máximo/mínimo sea fácil de extraerlo.  
\newline
\newline
Por lo tanto, \tadNombre{Cola de Prioridad($\alpha$, $<_{\alpha}$)} se representa con \textit{heap}.
\newline
\newline
Luego, el invariante de representación de un heap o condición de heap debe cumplir:
\newline
\newline
1. Ser un árbol binario perfectamente balanceado, es decir, la longitud de todas las ramas difiere en a lo sumo 1
\newline
\newline
2. La clave (prioridad) de cada nodo es mayor o igual que la de sus hijos, si los tiene
\newline
\newline
3. Todo subárbol es un heap
\newline
\newline
4. (No obligatorio): Es \textit{izquierdista}, o sea, el último nivel está lleno desde la izquierda, es decir, las ramas que le falten hijos deben estar todas a la derecha (o todos las hojas deben estar juntos a la izquierda del árbol)
\newline
\newline
\textbf{Observación:} Un \textit{heap} no es un \textit{ABB} ni una estructura totalmente ordenada.
\subsection{Operaciones sobre un (max-) heap}

Básicamente, las mismas que tenemos definidas en el TAD \tadNombre{Cola de Prioridad}.
\newline
\newline
\textbf{.} Vacia: Crea un heap vacio
\newline
\newline
\textbf{.} Próximo: Devuelve el elemento de máxima prioridad, sin modificar el heap.
\newline
\newline
\textbf{.} Encolar: Agrega un nuevo elemento, hay que reestablecer el invariante.
\newline
\newline
\textbf{.} Desencolar: Elimina el elemento de máxima prioridad, hay que reestablecer el invariante.
\newpage
\subsection{Implementación de heaps}
Todas las representaciones usadas para árboles binarios son admisibles
\newline
\newline
\textbf{.} Representación con punteros, eventualmente hijo-padre.
\newline
\newline
\textbf{.} Representación con arrays. En este caso, decimos que es una \textit{representación implícita} porque la posición de los elementos en el arreglo nos va a dar información/significado acerca de cual es la relación entre esos elementos.

\subsubsection{Representación con arrays}

Cada nodo $v$ es almacenado en la posición $p(v)$.
\newline
\newline
\textbf{.} Si $v$ es la raíz, entonces $p(v)$ = 0
\newline
\newline
\textbf{.} Si $v$ es hijo izquierdo de $u$, entonces $p(v)$ = 2$p(u)$ + 1 
\newline
\newline
\textbf{.} Si $v$ es hijo derecho de $u$, entonces $p(v)$ = 2$p(u)$ + 2
\newline
\newline
A continuación, vemos un ejemplo de cómo representar un \textit{heap} en un arreglo utilizando las funciones descriptas.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{heap-1}
	\label{drivers1}
\end{figure}
\newline
\newline
Veamos las \textbf{ventajas} de esta representación.
\newline
\newline
\textbf{.} Son muy eficientes en términos de espacio.
\newline
\newline
\textbf{.} Facilidad de navegación. 
\newline
\newline
Sea un padre $i$ ubicado en la posición $i$ e hijos $j_{izq}$ y $j_{der}$. Entonces:
\newline
\newline
$j_{izq}$ = $2i$ + 1
\newline
\newline
$j_{der}$ = $2i$ + 2
\newline
\newline
Ahora, sea un hijo $i$ ubicado en la posición $i$, entonces el padre $j$ se ubica en la posición $j$ = $\displaystyle \frac{i-1}{2}$ con $j$ parte entera
\newline
\newline
La principal \textbf{desventaja} es que trabajamos con una implementación estática y puede ser necesario duplicar o achicar el arreglo a medida que se agregan/eliminan elementos.
\subsection{Algoritmos}

Ahora vamos a ver los algoritmos para las operaciones sobre \textit{heaps}. Todos los algoritmo sirven para implementaciones sobre arrays o punteros. 
\newline
\newline
Comencemos viendo el algoritmo para calcular el \textbf{próximo}.
\newline
\newline
El elemento de máxima prioridad está en la posición 0 del arreglo.
\newline
\newline
Operación de costo constante $O(1)$.
\newline
\newline
Continuamos viendo el algoritmo para \textbf{encolar} un elemento.
\newline
\newline
Veamos, primero el siguiente ejemplo:
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{heap-2}
	\label{drivers1}
\end{figure}
\newline
\newline
Supongamos que queremos insertar el nodo número 15.
\newline
\newline
En la imagen \textit{a)} observamos que como un heap debe ser \textit{izquierdista}, la única donde podemos insertar al nodo es en la posición indicada.
\newline
\newline
Una vez insertado el nodo, vemos que no se cumple el invariante de \textit{heap} porque 7 $<$ 15. Entonces, intercambio sus posiciones como se muestra en la imagen \textit{b)}.
\newline
\newline
Una vez realizado el intercambio, observamos nuevamente que no se cumple el invariante de \textit{heap} porque 10 $<$ 15 e intercambio sus posiciones como se muestra en la imagen \textit{c)}. 
\newline
\newline
Por último, una vez realizado el intercambio, observamos que ahora si se cumple el invariante de \textit{heap} porque 15 $<$ 20 y finalizamos con la operación de inserción como se muestra en la imagen \textit{d)}. 
\newline
\newline
\newline
\newline
\newline
\newline
Veamos, ahora formalmente, los pasos para el algoritmo de inserción.
\newline
\newline
\textbf{.} Insertar elemento al final del heap
\newline
\newline
\textbf{.} Subir (\textit{sift-up}) elemento. La operación \textit{subir} se basa en que mientras el elemento al que se le aplica la función no sea la raíz y la prioridad del elemento sea mayor a la prioridad del padre, intercambiamos la posición del elemento con la del padre.
\newline
\newline
Ahora, veamos el algoritmo de \textbf{desencolar}. Notemos que cuando queremos desencolar queremos eliminar al elemento de mayor prioridad, es decir, queremos eliminar a la raíz.
\newline
\newline
Veamos el siguiente ejemplo:
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{heap-3}
	\label{drivers1}
\end{figure}
\newline
\newline
Luego de eliminar a la raíz, observamos en la imagen \textit{a)} que el nodo 6 no puede estar en su posición porque se romple la condición de heap.
\newline
\newline
Entonces, aprovechando que el nodo 20 que se encuentra en la raíz se va a eliminar, ocupo su posición con el nodo 6 como se muestra en la imagen \textit{b)}.
\newline
\newline
Ahora, como no se cumple el invariante de $heap$ porque 6 $<$ 15 y 10, tengo que colocar en la posición que corresponda al 6 para volver a tener un heap. Para ello, comparo al nodo 6 con el máximo de sus hijos (en este caso, el máximo es el 15) y los intercambio como se muestra en la imagen \textit{c)}.
\newline
\newline
Por último, podemos observar que luego de este intercambio, tampoco se cumple el invariante de heap porque 6 $<$ 14 y 13. Entoces, realizo la misma operación que en el caso anterior y lo intercambio con el 14. Finalmente, tenemos nuevamente un \textit{heap} como se muestra en la imagen \textit{d)}.
\newline
\newline
Ahora, veamos formalmente el algoritmo de \textbf{desencolar}.
\newline
\newline
\textbf{.} Reemplazar el primer elemento con la última hoja y eliminar la última hoja.
\newline
\newline
\textbf{.} Bajar la raíz. La operación \textit{bajar} se basa en que mientras el elemento no sea hoja y su prioridad sea menor a alguna de sus hijos, lo intercambiamos con el hijo de mayor prioridad.
\newpage
\subsection{Costos}

Tanto para encolar como para desencolar, los costos son proporcionales a la altura del $heap$, que es logarítmico. Por lo tanto, la complejidad de las operaciones mencionadas es $O(log(n))$.

\subsection{Array2Heap}

Dado un array $arr$, lo transforma en un array que representa un $heap$ a través de una permutación de sus elementos.
\newline
\newline
El algoritmo más básico se basa en recorrer todo el arreglo y para elemento, ir encolandolo (función con costo $O(log(n)$) en un nuevo arreglo de manera que preserva el invariante de $heap$.
\newline
\newline
Por lo tanto, el costo de esta operación, utilizando la \textit{aproximación de Stirling} ($ln(n!)$ $\approx$ $n~ln(n) - n$) es:
\newline
\newline
$\displaystyle \sum_{i=1}^{n} lg(i)$ = $lg(n!)$ = $\displaystyle \frac{ln(n!)}{ln(2)}$ $\approx$ $\displaystyle \frac{1}{ln(2)}$ . ($n~ln(n) - n$) = $\Theta(n~log(n))$   
\newline
\newline
¿Podemos mejorar la complejidad de esta operación?
\newline
\newline
El \textbf{algoritmo de Floyd} está basado en la idea de aplicar la operación \textit{bajar} a árboles binarios tales que los hijos de la raíz son raices de heaps.
\newline
\newline
Progresivamente, se \textit{heapifican} (\textit{heapify}) los subárboles con raíz en el penúltimo nivel, luego los del antepenúltimo, etc.
\newline
\newline
Esta algoritmo tiene una estrategia \textit{bottom-up}.
\newline
\newline
Veamos el siguiente ejemplo:
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{floyd-1}
	\label{drivers1}
\end{figure}
\newline
\newline
Observemos que de acuerdo a la implementación de heaps en arreglos, nos queda definido como en el árbol de la imagen. 
\newline
\newline
Luego, aplicamos el algoritmo de floyd desde las hojas. Como todas las hojas cumplen el invariante de $heap$, continuamos por el penúltimo nivel.
\newline
\newline
Una vez situados en el penúltimo nivel, comenzamos a analizar de derecha a izquierda.
\newline
\newline
El árbol que tiene como raíz al nodo 64 es un $max-heap$.
\newpage
Luego, continuamos viendo el árbol que tiene como raíz al nodo 23. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 23 y lo intercambiamos con el 68 para que sea un $heap$.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{floyd-2}
	\label{drivers1}
\end{figure}
\newline
\newline
Ahora, continuamos viendo el árbol que tiene como raíz al nodo 67. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 67 y lo intercambiamos con el 89 para que sea un $heap$.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{floyd-3}
	\label{drivers1}
\end{figure}
\newpage
Avanzamos un nivel más hacia arriba. Continuamos viendo el árbol que tiene como raíz al nodo 4. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 4 y lo intercambiamos con el 64 para que sea un $heap$. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 4 y lo intercambiamos con el 39 para que sea un $heap$.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{floyd-4}
	\label{drivers1}
\end{figure}
\newline
\newline
Continuamos viendo el árbol que tiene como raíz al nodo 5. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 5 y lo intercambiamos con el 89 para que sea un $heap$. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 5 y lo intercambiamos con el 67 para que sea un $heap$.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{floyd-5}
	\label{drivers1}
\end{figure}
\newpage
Avanzamos un nivel más hacia arriba y tenemos que considerar todo el árbol. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 66 y lo intercambiamos con el 89 para que sea un $heap$. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 66 y lo intercambiamos con el 68 para que sea un $heap$. Como no cumple la condición de heap, aplicamos la operación \textit{bajar} al nodo 66 y lo intercambiamos con el 67 para que sea un $heap$.
\newline
\begin{figure}[h]
	\centering
\includegraphics[width=0.6\textwidth]{floyd-6}
	\label{drivers1}
\end{figure}
\newline
\newline
Podemos observar ahora como el arreglo obtenido es una permutación del arreglo original.

\subsection{Análisis del Algoritmo de Floyd}

Ahora, analicemos la complejidad del algoritmo de Floyd.
\newline
\newline
En el caso peor, cada llamada a bajar realiza el máximo número de intercambios.
\newline
\newline
Supongamos un heap con $n$ = $2^{k} - 1$ nodos, es decir, un arbol binario completo de altura $k$. Entonces:
\newline
\newline
\textbf{.} En el último nivel hay $\displaystyle \frac{n+1}{2}$ hojas
\newline
\newline
\textbf{.} En el último nivel hay $\displaystyle \frac{n+1}{4}$ hojas
\newline
\newline
\textbf{.} En el último nivel hay $\displaystyle \frac{n+1}{8}$ hojas
\newline
\newline
Y así sucesivamente ...
\newline
\newline
Por lo tanto, una llamada de \textit{bajar} sobre un nodo de nivel $i$, provoca como máximo $k-i$ intercambios. Es decir, 1 intercambio si $i$ es el penúltimo nivel, 2 si $i$ es el antepenúltimo nivel, $k-1$ si $i=1$. Entonces:
\newline
\newline
\# max de intercambios = (\# nodos en el penúltimo nivel) . 1 + (\# nodos en el antepenúltimo nivel) . 2 + ... + (\# nodos en el nivel 2) . ($k-2$) + (\# nodos en el nivel 1) . ($k-1$) con $k$ = $log(n+1)$. Es decir: 
\newline
\newline
\# max de intercambios = ($\displaystyle \frac{n+1}{4}$) . 1 + ($\displaystyle \frac{n+1}{8}$) . 2 + ... + 2 . ($log(n+1) - 2$) + 1 . ($log(n+1) - 1$) 
\newline
\newline
\# max de intercambios = $\displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{n+1}{2^{i}}(i-1)$
\newline
\newline
\# max de intercambios = $(n+1)$ $\displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{i-1}{2^{i}}$
\newline
\newline
\# max de intercambios = $(n+1)$ ( $\displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{i}{2^{i}}$ - $\displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{1}{2^{i}}$)
\newline
\newline
\newline
Ahora, considerando que:
\newline
\newline
$\displaystyle \sum_{i=2}^{\infty} \displaystyle \frac{i}{2^{i}}$ $=$ $\displaystyle \frac{3}{2}$ y $\displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{1}{2^{i}}$ $>$ 0
\newline
\newline
Deducimos que:
\newline
\newline
\# max de intercambios = $(n+1)$ $\displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{i-1}{2^{i}}$ $<$ $(n+1)$ ($\displaystyle \frac{3}{2} - \displaystyle \sum_{i=2}^{log(n+1)} \displaystyle \frac{1}{2^{i}}$) $<$ $\displaystyle \frac{3}{2}$$(n+1)$
\newline
\newline
\newline
Lo que implica que \# max de intercambios = $O(n)$
\newpage

\section{Clase Teórica 09. Ordenamiento (Sorting)}

\subsection{El problema del ordenamiento}

Queremos ordenar un arreglo[$\alpha$] $\to$ arreglo[$\alpha$] donde $\alpha$ es un tipo tal que está definida la relación $<_{\alpha}$
\newline
\newline
Tambien nos vamos a enfocar en el ordenamiento en memoria secundaria como por ejemplo utilizado en grandes archivos. 

\subsection{Selection Sort}

Este algoritmo consiste en:
\newline
\newline
Repetir para las posiciones $i$ del arreglo:
\newline
\newline
\textbf{.} Seleccionar el mínimo elemento que se encuentra entre la posición actual ($i$-esima) y el final.
\newline
\newline
\textbf{.} Ubicarlo en la posición $i$, intercambiandolo con el ocupante original de esa posición.
\newline
\newline
Luego, en cada iteración, el algoritmo respeta el siguiente \textbf{invariante}:
\newline
\newline
\textbf{.} Los elementos entre la posición 0 y la posición $i$ son los $i+1$ elementos más pequeños del arreglo original.
\newline
\newline
\textbf{.} Los elementos entre la posición 0 y la posición $i$ se encuentran ordenados.
\newline
\newline
\textbf{.} El arreglo es una permutación del arreglo original (o sea, los elementos entre las posiciones $i+1$ y $n-1$ son los $n-i-1$ elementos más grandes del arreglo original).
\newline
\newline
Y donde presentamos el siguiente \textbf{pseudocódigo}:
\newline
\newline
Para $i$ desde 0 hasta $n-2$ hacer:
\newline
\newline
\TipoVariable{min $\leftarrow$ seleccionar\_min($i, n-1$)}
\newline
\newline
\TipoVariable{intercambiar(i, min)}
\newline
\newline
Luego, la función \TipoVariable{seleccionar\_min($i, n-1$)} tiene el siguiente pseudocódigo:
\newline
\newline
\TipoVariable{min $\leftarrow$ $i$}
\newline
\newline
Para $j$ desde $i+1$ hasta $n-1$ hacer:
\newline
\newline
si \TipoVariable{a[j] < a[min]} entonces \TipoVariable{min $\leftarrow$ j}  
\newline
\newline
Ahora, veamos una versión recursiva del mismo algoritmo.
\newline
\newline
Para ordenar un arreglo de posiciones $i$, ..., $n-1$ debemos:
\newline
\newline
\textbf{.} Seleccionar el mínimo elemento del arreglo.
\newline
\newline
\textbf{.} Ubicarlo en la posición $i$ intercambiandolo con el ocupante original de esa posición.
\newline
\newline
\textbf{.} Ordenarlo a través del mismo algoritmo el arreglo de posiciones $i+1$, ..., $n-1$
\newpage

\subsubsection{Complejidad}

Para calcular la complejidad de este algoritmo alcanza con contar la cantidad de comparaciones. 
\newline
\newline
Entonces, si queremos ordenar un arreglo de $n$ elementos, tenemos $n-1$ ejecuciones del ciclo principal y en la $i$-esima iteración, tenemos que encontrar el mínimo entre $n-i$ elementos y por lo tanto, necesitamos $n-i-1$ comparaciones.
\newline
\newline
Por lo tanto, el costo de este algoritmo es:
\newline
\newline
$\displaystyle \sum_{i=0}^{n-2}$ $(n-i-1)$ = $\displaystyle \sum_{i=1}^{n-1}$ $\displaystyle \frac{n~(n-1)}{2}$ $\approx$ $O(n^{2})$
\newline
\newline
\newline
Observemos que el costo no depende del eventual ordenamiento parcial o total del arreglo, es decir, en todos los casos, el costo asociado es $O(n^{2})$.

\subsection{Insertion Sort}

Este algoritmo consiste en:
\newline
\newline
Repetir para las posiciones sucesivas $i$ del arreglo:
\newline
\newline
\textbf{.} Insertar el $i$-esimo elemento en la posición que le corresponde del arreglo 0,...,$i$.
\newline
\newline
Luego, en cada iteración, el algoritmo respeta el siguiente \textbf{invariante}:
\newline
\newline
\textbf{.} Los elementos entre la posición 0 y la posición $i$ son los elementos que ocupaban las posiciones 0 a $i$ del arreglo original.
\newline
\newline
\textbf{.} Los elementos entre la posición 0 y la posición $i$ se encuentran ordenados
\newline
\newline
\textbf{.} El arreglo es una permutación del arreglo original, o sea, que los elementos de las posiciones $i+1$ hasta $n-1$ son los que ocupaban esas posiciones en el arreglo original
\newline
\newline































